{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from statistics import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, random, glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch_geometric.data import Data\n",
    "import h5py, torch, cv2\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, optim\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/dataset/nyu_depth_v2/official_splits\", transform=None, ext=\"jpg\", mode='train', threshold=0.06):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.filenames = glob.glob(os.path.join(self.data_path, mode, '**', '*.{}'.format(ext)), recursive=True)\n",
    "        self.pt_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_embedding/nyu_depth_v2/official_splits\"\n",
    "        self.depth_map_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_map/nyu_depth_v2/official_splits\"\n",
    "#         self.depth_map_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/adabins/depth_map/nyu_depth_v2/official_splits\"\n",
    "        self.sg_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2/official_splits\"\n",
    "\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "\n",
    "        # image path\n",
    "        img_path = self.filenames[idx]\n",
    "        # get the image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # get the relative path\n",
    "        relative_path = os.path.relpath(img_path, self.data_path)\n",
    "        # get depth embedding path\n",
    "        depth_emb_path = os.path.join(self.pt_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "        # get depth map path\n",
    "        depth_path = os.path.join(self.mode, self.depth_map_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "\n",
    "        #get the scene graph path\n",
    "        scenegraph_path = os.path.join(self.sg_path, '{}.h5'.format(relative_path.split('.')[0]))\n",
    "\n",
    "\n",
    "\n",
    "        ## Depth Embedding\n",
    "        depth_emb = self.normalize(torch.load(depth_emb_path))\n",
    "\n",
    "        ## depth map\n",
    "        depth_map = self.normalize(torch.load(depth_path))\n",
    "\n",
    "        ## get the actual depth\n",
    "        actual_depth_path = img_path.replace(\"rgb\", \"sync_depth\").replace('.jpg', '.png')\n",
    "        actual_depth = Image.open(actual_depth_path)\n",
    "\n",
    "        \n",
    "        # the resize size\n",
    "        target_size = (25, 25)\n",
    "\n",
    "\n",
    "        with h5py.File(scenegraph_path, 'r') as h5_file:\n",
    "            loaded_output_dict = {key: torch.tensor(np.array(h5_file[key])) for key in h5_file.keys()}\n",
    "\n",
    "\n",
    "        probas = loaded_output_dict['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_sub = loaded_output_dict['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_obj = loaded_output_dict['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "        \n",
    "        \n",
    "        keep = torch.logical_and(probas.max(-1).values > self.threshold, \n",
    "                                torch.logical_and(probas_sub.max(-1).values > self.threshold, probas_obj.max(-1).values > self.threshold))\n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        sub_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['sub_boxes'][0, keep], img.size)\n",
    "        obj_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['obj_boxes'][0, keep], img.size)\n",
    "        relations = loaded_output_dict['rel_logits'][0, keep]\n",
    "\n",
    "        valid_sub_bboxes = self.validate_bounding_boxes(sub_bboxes_scaled, img.size, target_size)\n",
    "        valid_obj_bboxes = self.validate_bounding_boxes(obj_bboxes_scaled, img.size, target_size)\n",
    "\n",
    "        # Combine validity of subject and object bounding boxes\n",
    "        valid_pairs = torch.tensor([vs and vo for vs, vo in zip(valid_sub_bboxes, valid_obj_bboxes)], dtype=torch.bool)\n",
    "\n",
    "        # Apply the updated keep mask\n",
    "        sub_bboxes_scaled = sub_bboxes_scaled[valid_pairs]\n",
    "        obj_bboxes_scaled = obj_bboxes_scaled[valid_pairs]\n",
    "        relations = relations[valid_pairs]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sub_idxs, nodes1 = self.assign_index(sub_bboxes_scaled, [], threshold=0.7)\n",
    "        obj_idxs, nodes2 = self.assign_index(obj_bboxes_scaled, nodes1, threshold=0.7)\n",
    "        \n",
    "        all_idxs = sub_idxs + obj_idxs\n",
    "        bbox_lists = torch.concat((sub_bboxes_scaled, obj_bboxes_scaled), dim=0)\n",
    "        \n",
    "        unique_idxs = set()\n",
    "        filtered_idxs = []\n",
    "        filtered_bboxes = []\n",
    "\n",
    "        for idx, bbox in zip(all_idxs, bbox_lists):   \n",
    "            if idx not in unique_idxs:\n",
    "                unique_idxs.add(idx)\n",
    "                filtered_idxs.append(idx)\n",
    "                filtered_bboxes.append(bbox.tolist())  \n",
    "\n",
    "        # Sort the filtered indices along with their corresponding bounding boxes\n",
    "        sorted_wrapped = sorted(zip(filtered_idxs, filtered_bboxes), key=lambda x: x[0])\n",
    "        sorted_idxs, sorted_bboxes = zip(*sorted_wrapped)\n",
    "\n",
    "        # Convert them back to torch tensors\n",
    "        sorted_idxs = torch.tensor(sorted_idxs, dtype=torch.long)\n",
    "        sorted_bboxes = torch.tensor(sorted_bboxes, dtype=torch.int32)        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Apply transform to image and depth\n",
    "        img = self.transform(img)\n",
    "        img = self.normalize(img)\n",
    "        actual_depth = self.transform(actual_depth).float()\n",
    "        actual_depth = self.normalize(actual_depth)\n",
    "        \n",
    "        device = 'cpu'\n",
    "        \n",
    "#         pooled_images = self.pool_visual_content_and_depth(\n",
    "#             sorted_bboxes=sorted_bboxes,\n",
    "#             embedding=img,\n",
    "#             target_size=target_size).to(device)\n",
    "        \n",
    "#         pooled_depths = self.pool_visual_content_and_depth(\n",
    "#             sorted_bboxes=sorted_bboxes,\n",
    "#             embedding=depth_emb[0],\n",
    "#             target_size=target_size).to(device)\n",
    "        \n",
    "#         pooled_act_depths = self.resize_depth_map(\n",
    "#             sorted_bboxes=sorted_bboxes,\n",
    "#             embedding=actual_depth.squeeze(0),\n",
    "#             target_size=target_size,\n",
    "#             method='mode').to(device)\n",
    "\n",
    "        pooled_images = self.linear_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=img,\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        pooled_depths = self.linear_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=depth_emb[0],\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        \n",
    "#         pooled_depth_maps = self.resize_depth_map(\n",
    "#             sorted_bboxes=sorted_bboxes,\n",
    "#             embedding=depth_map.squeeze(0),\n",
    "#             target_size=target_size,\n",
    "#             method='linear').to(device)\n",
    "\n",
    "    ## Adabins\n",
    "        pooled_depth_maps = self.resize_depth_map(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=depth_map,\n",
    "            target_size=target_size,\n",
    "            method='linear').to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pooled_act_depths = self.resize_depth_map(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=actual_depth.squeeze(0),\n",
    "            target_size=target_size,\n",
    "            method='linear').to(device)\n",
    "        \n",
    "        act_depths = self.resize_depth_map(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=actual_depth.squeeze(0),\n",
    "            target_size=target_size,\n",
    "            method='real')\n",
    "        \n",
    "#         act_depths = torch.tensor(act_depths).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(depth_map, np.ndarray):\n",
    "            depth_map = torch.from_numpy(depth_map)\n",
    "        \n",
    "        \n",
    "        # node embedding\n",
    "        node_embeddings = torch.cat([pooled_images, pooled_depths], dim=1).to(device)\n",
    "        \n",
    "        edge_index = torch.tensor([sub_idxs, obj_idxs]).to(device)\n",
    "        \n",
    "#         edge_embeddings = torch.tensor(relations, dtype=torch.float).to(device)\n",
    "        edge_embeddings = relations.clone().detach().float().to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        gnndata = Data(x=node_embeddings, edge_index=edge_index, edge_attr=edge_embeddings)\n",
    "\n",
    "\n",
    "        ## return data\n",
    "        data = {\n",
    "            'img_path': img_path,\n",
    "            'image': img,\n",
    "            'depth_emb': depth_emb,\n",
    "            'depth_map': depth_map,\n",
    "            'depth': actual_depth,\n",
    "            'pooled_depth_maps': pooled_depth_maps,\n",
    "            'pooled_act_depths': pooled_act_depths,\n",
    "            'act_depths': act_depths,\n",
    "            'bboxs': filtered_bboxes,\n",
    "            'gnndata': gnndata\n",
    "        }\n",
    "        \n",
    "        self.cache[idx] = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "\n",
    "        x_c, y_c, w, h = x.unbind(1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    \n",
    "        return torch.stack(b, dim=1)\n",
    "    \n",
    "    def rescale_bboxes(self, out_bbox, size):\n",
    "\n",
    "        img_w, img_h = size\n",
    "        b = self.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "        \n",
    "        b = torch.round(b).int()\n",
    "\n",
    "        b[:, 0] = torch.clamp(b[:, 0], min=0, max=img_w)\n",
    "        b[:, 1] = torch.clamp(b[:, 1], min=0, max=img_h)\n",
    "        b[:, 2] = torch.clamp(b[:, 2], min=0, max=img_w)\n",
    "        b[:, 3] = torch.clamp(b[:, 3], min=0, max=img_h)\n",
    "\n",
    "        return b\n",
    "\n",
    "    def validate_bounding_boxes(self, bboxes, img_size, target_size):\n",
    "        \"\"\"Return a list of booleans indicating whether each bounding box is valid.\"\"\"\n",
    "        valid_bboxes = []\n",
    "        img_w, img_h = img_size\n",
    "        \n",
    "        t_w, t_h = target_size\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            if x1 < 0: x1 = 0\n",
    "            if y1 < 0: y1 = 0\n",
    "            if x2 > img_w: x2 = img_w\n",
    "            if y2 > img_h: y2 = img_h\n",
    "            \n",
    "            if (x2 - x1) >= t_w and (y2 - y1) >= t_h:\n",
    "                valid_bboxes.append(True)\n",
    "            else:\n",
    "                valid_bboxes.append(False)\n",
    "\n",
    "        return valid_bboxes\n",
    "\n",
    "\n",
    "    \n",
    "    def pool_visual_content_and_depth(self, sorted_bboxes, embedding, target_size=(50, 50)):\n",
    "\n",
    "        pool = nn.AdaptiveAvgPool2d(target_size)\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            cropped_emb = embedding[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            if cropped_emb.dim() == 2:\n",
    "                cropped_emb = cropped_emb.unsqueeze(0)  \n",
    "\n",
    "            pooled_emb = pool(cropped_emb.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            flattened_emb = pooled_emb.view(-1)\n",
    "            \n",
    "            pooled_embs.append(flattened_emb)\n",
    "\n",
    "\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "\n",
    "\n",
    "    def linear_visual_content_and_depth(self, sorted_bboxes, embedding, target_size=(50, 50)):\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            # Crop the embedding to the bounding box\n",
    "            cropped_emb = embedding[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            # If the cropped embedding is 2D, add a channel dimension\n",
    "            if cropped_emb.dim() == 2:\n",
    "                cropped_emb = cropped_emb.unsqueeze(0)\n",
    "\n",
    "            # Bilinear downsampling using F.interpolate\n",
    "            pooled_emb = F.interpolate(cropped_emb.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "            # Flatten the downsampled embedding\n",
    "            flattened_emb = pooled_emb.view(-1)\n",
    "\n",
    "            # Append the flattened embedding to the list\n",
    "            pooled_embs.append(flattened_emb)\n",
    "\n",
    "        # Stack all the pooled embeddings or return an empty tensor if none\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "    \n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "\n",
    "        x1, y1, x2, y2 = box1\n",
    "        x3, y3, x4, y4 = box2\n",
    "\n",
    "        # Calculate intersection coordinates\n",
    "        x_inter1 = max(x1, x3)\n",
    "        y_inter1 = max(y1, y3)\n",
    "        x_inter2 = min(x2, x4)\n",
    "        y_inter2 = min(y2, y4)\n",
    "\n",
    "        # Calculate intersection dimensions\n",
    "        width_inter = max(0, x_inter2 - x_inter1)\n",
    "        height_inter = max(0, y_inter2 - y_inter1)\n",
    "\n",
    "        # Calculate intersection area\n",
    "        area_inter = width_inter * height_inter\n",
    "\n",
    "        # Calculate areas of the input boxes\n",
    "        width_box1 = abs(x2 - x1)\n",
    "        height_box1 = abs(y2 - y1)\n",
    "        area_box1 = width_box1 * height_box1\n",
    "\n",
    "        width_box2 = abs(x4 - x3)\n",
    "        height_box2 = abs(y4 - y3)\n",
    "        area_box2 = width_box2 * height_box2\n",
    "\n",
    "        # Calculate union area\n",
    "        area_union = area_box1 + area_box2 - area_inter\n",
    "\n",
    "        # Calculate IoU\n",
    "        if area_union == 0:\n",
    "            return 0  # avoid division by zero\n",
    "        iou = area_inter / area_union\n",
    "\n",
    "        return iou\n",
    "    \n",
    "    \n",
    "    def assign_index(self, bounding_boxes, nodes, threshold=0.5):\n",
    "        indices = []\n",
    "        existing_boxes = nodes\n",
    "\n",
    "        for box in bounding_boxes:\n",
    "            found_match = False\n",
    "            for idx, existing_box in enumerate(existing_boxes):\n",
    "                if self.calculate_iou(box, existing_box) > threshold:\n",
    "                    indices.append(idx)\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            if not found_match:\n",
    "                existing_boxes.append(box)\n",
    "                indices.append(len(existing_boxes) - 1)\n",
    "\n",
    "        return indices, existing_boxes\n",
    "    \n",
    "    \n",
    "    def normalize(self, tensor):\n",
    "        tensor_min = tensor.min()\n",
    "        tensor_max = tensor.max()\n",
    "        normalized_tensor = (tensor - tensor_min) / ((tensor_max - tensor_min) + 1e-8)\n",
    "        return normalized_tensor\n",
    "    \n",
    "    \n",
    "### here\n",
    "    def resize_depth_map(self, sorted_bboxes, embedding, target_size=(25, 25), method='linear'):\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            cropped_emb = embedding[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            if method == 'linear':\n",
    "                pooled_emb = self.downsize_depth_map_bilinear(cropped_emb, target_size)\n",
    "                flattened_emb = pooled_emb.view(-1)\n",
    "            elif method == 'mode':\n",
    "                pooled_emb = self.downsize_depth_map_mode(cropped_emb, target_size)\n",
    "                flattened_emb = pooled_emb.view(-1)\n",
    "            else:\n",
    "                pooled_emb = cropped_emb\n",
    "                flattened_emb = pooled_emb.reshape(-1)\n",
    "\n",
    "#             flattened_emb = pooled_emb.view(-1)\n",
    "            \n",
    "            pooled_embs.append(flattened_emb)\n",
    "        \n",
    "        \n",
    "        if method == 'real':\n",
    "            return pooled_embs\n",
    "\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "    \n",
    "    \n",
    "    def downsize_depth_map_mode(self, depth_map, new_size):\n",
    "        \n",
    "        original_height, original_width = depth_map.shape[-2], depth_map.shape[-1]\n",
    "        new_height, new_width = new_size\n",
    "\n",
    "        window_height = original_height // new_height\n",
    "        window_width = original_width // new_width\n",
    "\n",
    "        downsized_map = torch.zeros((new_height, new_width), dtype=depth_map.dtype, device=depth_map.device)\n",
    "\n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                # Define the window boundaries\n",
    "                start_row = i * window_height\n",
    "                end_row = start_row + window_height\n",
    "                start_col = j * window_width\n",
    "                end_col = start_col + window_width\n",
    "\n",
    "                # Extract the window\n",
    "                window = depth_map[start_row:end_row, start_col:end_col]\n",
    "\n",
    "                # Flatten the window\n",
    "                flat_window = window.flatten().cpu().numpy()  # Convert to numpy for mode calculation\n",
    "\n",
    "                downsized_map[i, j] = mode(flat_window)\n",
    "\n",
    "        return downsized_map\n",
    "    \n",
    "    def downsize_depth_map_bilinear(self, depth_map, new_size):\n",
    "\n",
    "        if isinstance(depth_map, np.ndarray):\n",
    "            depth_map = torch.from_numpy(depth_map)\n",
    "\n",
    "        if depth_map.dim() == 2:\n",
    "            depth_map = depth_map.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "\n",
    "        new_height, new_width = new_size\n",
    "\n",
    "        resized_map = F.interpolate(depth_map, size=(new_height, new_width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return resized_map.squeeze(0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \n",
    "    \n",
    "    batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    img_path = [item['img_path'] for item in batch]\n",
    "    \n",
    "    # Handle images, depth maps, and other tensors separately\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    depth_embs = torch.stack([item['depth_emb'] for item in batch])\n",
    "    depth_maps = torch.stack([item['depth_map'] for item in batch])\n",
    "    depths = torch.stack([item['depth'] for item in batch])\n",
    "    \n",
    "#     pooled_act_depths = [torch.tensor(item['pooled_act_depths'], dtype=torch.int) for item in batch]\n",
    "    pooled_act_depths = [item['pooled_act_depths'].clone().detach().float() for item in batch]\n",
    "    pooled_depth_maps = [item['pooled_depth_maps'].clone().detach().float() for item in batch]\n",
    "#     act_depths = [item['act_depths'].clone().detach().float() for item in batch]\n",
    "    act_depths = []\n",
    "    for item in batch:\n",
    "        if isinstance(item['act_depths'], list):\n",
    "            act_depths.append([x.clone().detach().float() for x in item['act_depths']])\n",
    "        else:\n",
    "            act_depths.append(item['act_depths'].clone().detach().float())\n",
    "\n",
    "\n",
    "#     bboxs = torch.stack([item['bboxs'] for item in batch])\n",
    "    bboxs = [torch.tensor(item['bboxs'], dtype=torch.int) for item in batch]\n",
    "\n",
    "    gnndata_list = [item['gnndata'] for item in batch]\n",
    "\n",
    "    # Batch the graph data using PyTorch Geometric's Batch\n",
    "    graph_batch = Batch.from_data_list(gnndata_list)\n",
    "\n",
    "    return {\n",
    "        'img_path': img_path,\n",
    "        'image': images,\n",
    "        'depth_emb': depth_embs,\n",
    "        'depth_map': depth_maps,\n",
    "        'depth': depths,\n",
    "        'pooled_act_depths': pooled_act_depths,\n",
    "        'act_depths': act_depths,\n",
    "        'pooled_depth_maps': pooled_depth_maps,\n",
    "        'bboxs': bboxs,\n",
    "        'gnndata': graph_batch,  # Batched graph data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn as nn\n",
    "\n",
    "class DepthGNNModel(MessagePassing):\n",
    "    def __init__(self, node_features_size, edge_features_size, hidden_channels, output_size):\n",
    "        super(DepthGNNModel, self).__init__(aggr='add')\n",
    "\n",
    "        self.message_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_features_size + edge_features_size, 1024),  # Reduced size\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, hidden_channels)\n",
    "        )\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features_size + hidden_channels, 2048),  # Reduced size\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, output_size)  # Output a flattened 25x25 depth map\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        message_input = torch.cat([x_i, edge_attr, x_j], dim=-1)\n",
    "        return self.message_mlp(message_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        updated_node_features = torch.cat([x, aggr_out], dim=-1)\n",
    "        return self.node_mlp(updated_node_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './model_weights/current_linear_checkpoint.pth'\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint, starting evaluation from epoch 48\n"
     ]
    }
   ],
   "source": [
    "### load model\n",
    "node_features_size =  41875 # 82075 # \n",
    "edge_features_size = 52\n",
    "hidden_channels = 728\n",
    "output_size = 625 #1225 # \n",
    "\n",
    "gnn_model = DepthGNNModel(node_features_size, edge_features_size, hidden_channels, output_size).cuda()\n",
    "optimizer = optim.Adam(gnn_model.parameters(), lr=1e-4)\n",
    "\n",
    "model_check_point = load_checkpoint(gnn_model, optimizer, checkpoint_path)\n",
    "print(f\"Model loaded from checkpoint, starting evaluation from epoch {model_check_point + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DepthDataset(mode='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING ALL TESTDATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "\n",
    "    epsilon = 1e-8\n",
    "#     epsilon = 0\n",
    "    \n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val + epsilon)\n",
    "    \n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def from_flatten_map(flatten_map, depth_map_size, target_size, method='nearest'):\n",
    "    \n",
    "    # Ensure that flatten_map is a torch tensor\n",
    "    if not isinstance(flatten_map, torch.Tensor):\n",
    "        raise TypeError(\"flatten_map should be a PyTorch tensor.\")\n",
    "    \n",
    "    # Reshape flatten map to depth map size\n",
    "    depth_map = flatten_map.view(depth_map_size)  # Convert 1D to desired depth map size (H, W)\n",
    "\n",
    "    # Add batch and channel dimensions for interpolation (N, C, H, W)\n",
    "    depth_map = depth_map.unsqueeze(0).unsqueeze(0)  # Add batch dimension and channel dimension\n",
    "\n",
    "    # Perform interpolation to the target size\n",
    "    # Note: F.interpolate expects the input tensor format (N, C, H, W)\n",
    "    upsampled_map = F.interpolate(depth_map, size=target_size, mode=method)\n",
    "    \n",
    "    # Remove batch and channel dimensions\n",
    "    upsampled_map = upsampled_map.squeeze(0).squeeze(0)  # Remove batch and channel dimensions\n",
    "\n",
    "    return upsampled_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mae(pred, target):\n",
    "    \"\"\"Calculate Mean Absolute Error (MAE) between prediction and target.\"\"\"\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "def rmse(pred, target):\n",
    "    \"\"\"Calculate Root Mean Squared Error (RMSE) between prediction and target.\"\"\"\n",
    "    return torch.sqrt(torch.mean((pred - target) ** 2))\n",
    "\n",
    "def threshold_accuracy(pred, target, threshold):\n",
    "    \"\"\"Calculate the percentage of pixels where the prediction is within a certain threshold of the target.\"\"\"\n",
    "    ratio = torch.max(pred / target, target / pred)\n",
    "    return (ratio < threshold).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [05:00<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics for Each Output (excluding NaN values):\n",
      "reshaped_ground_truth:\n",
      "  MAE = 0.0464\n",
      "  RMSE = 0.1341\n",
      "  δ1 = 0.9152\n",
      "  δ2 = 0.9337\n",
      "  δ3 = 0.9414\n",
      "node_output:\n",
      "  MAE = 0.2533\n",
      "  RMSE = 0.3035\n",
      "  δ1 = 0.3358\n",
      "  δ2 = 0.5770\n",
      "  δ3 = 0.6572\n",
      "ada_output:\n",
      "  MAE = 0.3741\n",
      "  RMSE = 0.4416\n",
      "  δ1 = 0.1969\n",
      "  δ2 = 0.3263\n",
      "  δ3 = 0.4148\n",
      "ada_pool_output:\n",
      "  MAE = 0.3750\n",
      "  RMSE = 0.4447\n",
      "  δ1 = 0.2014\n",
      "  δ2 = 0.3321\n",
      "  δ3 = 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def check_for_nan(values, name=\"\"):\n",
    "    \"\"\"Check if any of the values in the list are NaN.\"\"\"\n",
    "    tensor_values = torch.tensor(list(values), dtype=torch.float32, device=device)\n",
    "    if torch.isnan(tensor_values).any():\n",
    "        print(f\"Warning: {name} contains NaN values!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_metrics(pred, target):\n",
    "    \"\"\"Compute selected metrics between prediction and target with additional checks.\"\"\"\n",
    "    # Avoid division by zero and ensure positive values for log calculation\n",
    "    pred = torch.clamp(pred, max=1, min=1e-6)\n",
    "    target = torch.clamp(target, max=1, min=1e-6)\n",
    "    \n",
    "    metrics = {\n",
    "        'mae': mae(pred, target).item(),\n",
    "        'rmse': rmse(pred, target).item(),\n",
    "        'delta_1': threshold_accuracy(pred, target, 1.25).item(),\n",
    "        'delta_2': threshold_accuracy(pred, target, 1.25 ** 2).item(),\n",
    "        'delta_3': threshold_accuracy(pred, target, 1.25 ** 3).item()\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_metrics = 5  # Updated number of metrics being calculated\n",
    "    total_metrics = {\n",
    "        'reshaped_ground_truth': torch.zeros(num_metrics, device=device),\n",
    "        'node_output': torch.zeros(num_metrics, device=device),\n",
    "        'ada_output': torch.zeros(num_metrics, device=device),\n",
    "        'ada_pool_output': torch.zeros(num_metrics, device=device)\n",
    "    }\n",
    "    valid_samples_count = {\n",
    "        'reshaped_ground_truth': 0,\n",
    "        'node_output': 0,\n",
    "        'ada_output': 0,\n",
    "        'ada_pool_output': 0\n",
    "    }\n",
    "    \n",
    "    bad_imgs = []\n",
    "    \n",
    "    gnn_model.eval()\n",
    "\n",
    "    for single_batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        gnndata = single_batch['gnndata'].to(device)\n",
    "        output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "        count = len(single_batch['act_depths'][0])\n",
    "        \n",
    "        stored_img = False\n",
    "        \n",
    "        for img_idx in range(count):\n",
    "\n",
    "            depth_map_size = (25, 25)  \n",
    "            bbox = single_batch['bboxs'][0][img_idx]\n",
    "\n",
    "            target_size = (\n",
    "                int(bbox[3] - bbox[1]),  # Height as integer\n",
    "                int(bbox[2] - bbox[0])   # Width as integer\n",
    "            )\n",
    "\n",
    "            # Ground truth\n",
    "            ground_truth = single_batch['act_depths'][0][img_idx]\n",
    "            ground_truth = ground_truth.reshape(target_size).to(device)\n",
    "\n",
    "            # Reshaped ground truth\n",
    "            reshaped_ground_truth = single_batch['pooled_act_depths'][0][img_idx]\n",
    "            reshaped_ground_truth = from_flatten_map(reshaped_ground_truth, depth_map_size, target_size, 'nearest').to(device)\n",
    "\n",
    "            # Node output\n",
    "            node_output = output[img_idx]\n",
    "            node_output = from_flatten_map(node_output, depth_map_size, target_size, 'nearest').to(device)\n",
    "\n",
    "            # Adabins output\n",
    "            ada_output = single_batch['depth_map'][0][bbox[1]:bbox[3], bbox[0]:bbox[2]].to(device)\n",
    "\n",
    "            # Downsampled Adabins output\n",
    "            ada_pool_output = single_batch['pooled_depth_maps'][0][img_idx]\n",
    "            ada_pool_output = from_flatten_map(ada_pool_output, depth_map_size, target_size, 'nearest').to(device)\n",
    "            \n",
    "            # Normalize tensors\n",
    "            ground_truth = normalize_tensor(ground_truth.float())\n",
    "            reshaped_ground_truth = normalize_tensor(reshaped_ground_truth.float())\n",
    "            node_output = normalize_tensor(node_output.float())\n",
    "            ada_output = normalize_tensor(ada_output.float())\n",
    "            ada_pool_output = normalize_tensor(ada_pool_output.float())\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics_reshaped_ground_truth = compute_metrics(reshaped_ground_truth, ground_truth)\n",
    "            metrics_node_output = compute_metrics(node_output, ground_truth)\n",
    "            metrics_ada_output = compute_metrics(ada_output, ground_truth)\n",
    "            metrics_ada_pool_output = compute_metrics(ada_pool_output, ground_truth)\n",
    "\n",
    "            # Only accumulate metrics if they do not contain NaN values\n",
    "            if not check_for_nan(metrics_reshaped_ground_truth.values(), \"reshaped_ground_truth\"):\n",
    "                total_metrics['reshaped_ground_truth'] += torch.tensor(list(metrics_reshaped_ground_truth.values()), device=device)\n",
    "                valid_samples_count['reshaped_ground_truth'] += 1\n",
    "                stored_img = True\n",
    "            \n",
    "            if not check_for_nan(metrics_node_output.values(), \"node_output\"):\n",
    "                total_metrics['node_output'] += torch.tensor(list(metrics_node_output.values()), device=device)\n",
    "                valid_samples_count['node_output'] += 1\n",
    "                stored_img = True\n",
    "\n",
    "            if not check_for_nan(metrics_ada_output.values(), \"ada_output\"):\n",
    "                total_metrics['ada_output'] += torch.tensor(list(metrics_ada_output.values()), device=device)\n",
    "                valid_samples_count['ada_output'] += 1\n",
    "                stored_img = True\n",
    "                \n",
    "            if not check_for_nan(metrics_ada_pool_output.values(), \"ada_pool_output\"):\n",
    "                total_metrics['ada_pool_output'] += torch.tensor(list(metrics_ada_pool_output.values()), device=device)\n",
    "                valid_samples_count['ada_pool_output'] += 1\n",
    "                stored_img = True\n",
    "                \n",
    "        if stored_img:\n",
    "            bad_imgs.append(single_batch['img_path'])\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            del ground_truth, reshaped_ground_truth, node_output, ada_output, ada_pool_output\n",
    "\n",
    "# Compute average metrics across all valid samples\n",
    "avg_metrics = {}\n",
    "for key in total_metrics:\n",
    "    if valid_samples_count[key] > 0:\n",
    "        avg_metrics[key] = total_metrics[key] / valid_samples_count[key]\n",
    "    else:\n",
    "        avg_metrics[key] = torch.tensor([float('nan')] * num_metrics, device=device)  # Set to NaN if no valid samples\n",
    "\n",
    "# Print the results\n",
    "metric_names = ['MAE', 'RMSE', 'δ1', 'δ2', 'δ3']\n",
    "\n",
    "print(\"Average Metrics for Each Output (excluding NaN values):\")\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key}:\")\n",
    "    for name, metric_value in zip(metric_names, value):\n",
    "        print(f\"  {name} = {metric_value.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "depth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
