{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GNN_for_Depth.data.dataset import DepthDataset\n",
    "# from GNN_for_Depth.data.utils import custom_collate\n",
    "# from GNN_forDepth.utils.criterion import SiLogLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# from torchvision.ops import RoIAlign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/fsml62/LLM_and_SGG_for_MDE/.depth/lib/python3.8/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home3/fsml62/LLM_and_SGG_for_MDE/.depth/lib/python3.8/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Clear cache to free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch_geometric.data import Data\n",
    "import h5py, torch, cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/dataset/nyu_depth_v2/official_splits\", transform=None, ext=\"jpg\", mode='train', threshold=0.06):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.filenames = glob.glob(os.path.join(self.data_path, mode, '**', '*.{}'.format(ext)), recursive=True)\n",
    "        self.pt_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_embedding/nyu_depth_v2/official_splits\"\n",
    "        self.depth_map_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_map/nyu_depth_v2/official_splits\"\n",
    "        self.sg_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2/official_splits\"\n",
    "\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # image path\n",
    "        img_path = self.filenames[idx]\n",
    "        # get the image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # get the relative path\n",
    "        relative_path = os.path.relpath(img_path, self.data_path)\n",
    "        # get depth embedding path\n",
    "        depth_emb_path = os.path.join(self.pt_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "        # get depth map path\n",
    "        depth_path = os.path.join(self.mode, self.depth_map_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "\n",
    "        #get the scene graph path\n",
    "        scenegraph_path = os.path.join(self.sg_path, '{}.h5'.format(relative_path.split('.')[0]))\n",
    "\n",
    "\n",
    "\n",
    "        ## Depth Embedding\n",
    "        depth_emb = self.normalize(torch.load(depth_emb_path))\n",
    "\n",
    "        ## depth map\n",
    "        depth_map = self.normalize(torch.load(depth_path))\n",
    "\n",
    "        ## get the actual depth\n",
    "        actual_depth_path = img_path.replace(\"rgb\", \"sync_depth\").replace('.jpg', '.png')\n",
    "        actual_depth = Image.open(actual_depth_path)\n",
    "\n",
    "\n",
    "\n",
    "        with h5py.File(scenegraph_path, 'r') as h5_file:\n",
    "            loaded_output_dict = {key: torch.tensor(np.array(h5_file[key])) for key in h5_file.keys()}\n",
    "\n",
    "\n",
    "        probas = loaded_output_dict['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_sub = loaded_output_dict['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_obj = loaded_output_dict['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "        \n",
    "        \n",
    "        keep = torch.logical_and(probas.max(-1).values > self.threshold, \n",
    "                                torch.logical_and(probas_sub.max(-1).values > self.threshold, probas_obj.max(-1).values > self.threshold))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sub_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['sub_boxes'][0, keep], img.size)\n",
    "        obj_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['obj_boxes'][0, keep], img.size)\n",
    "        relations = loaded_output_dict['rel_logits'][0, keep]\n",
    "\n",
    "        valid_sub_bboxes = self.validate_bounding_boxes(sub_bboxes_scaled, img.size)\n",
    "        valid_obj_bboxes = self.validate_bounding_boxes(obj_bboxes_scaled, img.size)\n",
    "\n",
    "        # Combine validity of subject and object bounding boxes\n",
    "        valid_pairs = torch.tensor([vs and vo for vs, vo in zip(valid_sub_bboxes, valid_obj_bboxes)], dtype=torch.bool)\n",
    "\n",
    "        # Apply the updated keep mask\n",
    "        sub_bboxes_scaled = sub_bboxes_scaled[valid_pairs]\n",
    "        obj_bboxes_scaled = obj_bboxes_scaled[valid_pairs]\n",
    "        relations = relations[valid_pairs]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sub_idxs, nodes1 = self.assign_index(sub_bboxes_scaled, [], threshold=0.7)\n",
    "        obj_idxs, nodes2 = self.assign_index(obj_bboxes_scaled, nodes1, threshold=0.7)\n",
    "        \n",
    "        all_idxs = sub_idxs + obj_idxs\n",
    "        bbox_lists = torch.concat((sub_bboxes_scaled, obj_bboxes_scaled), dim=0)\n",
    "        \n",
    "        unique_idxs = set()\n",
    "        filtered_idxs = []\n",
    "        filtered_bboxes = []\n",
    "\n",
    "        for idx, bbox in zip(all_idxs, bbox_lists):   \n",
    "            if idx not in unique_idxs:\n",
    "                unique_idxs.add(idx)\n",
    "                filtered_idxs.append(idx)\n",
    "                filtered_bboxes.append(bbox.tolist())  \n",
    "\n",
    "        # Sort the filtered indices along with their corresponding bounding boxes\n",
    "        sorted_wrapped = sorted(zip(filtered_idxs, filtered_bboxes), key=lambda x: x[0])\n",
    "        sorted_idxs, sorted_bboxes = zip(*sorted_wrapped)\n",
    "\n",
    "        # Convert them back to torch tensors\n",
    "        sorted_idxs = torch.tensor(sorted_idxs, dtype=torch.long)\n",
    "        sorted_bboxes = torch.tensor(sorted_bboxes, dtype=torch.int32)        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Apply transform to image and depth\n",
    "        img = self.transform(img)\n",
    "        img = self.normalize(img)\n",
    "        actual_depth = self.transform(actual_depth).float()\n",
    "        \n",
    "        device = 'cpu'\n",
    "        \n",
    "        \n",
    "        # pool_visual_content_and_depth(sorted_bboxes, embedding, target_size=(50, 50)):\n",
    "        target_size = (25, 25)\n",
    "#         target_size = (35, 35)\n",
    "        \n",
    "        pooled_images = self.pool_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=img,\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        pooled_depths = self.pool_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=depth_emb[0],\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        pooled_act_depths = self.pool_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=actual_depth,\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # node embedding\n",
    "        node_embeddings = torch.cat([pooled_images, pooled_depths], dim=1).to(device)\n",
    "        \n",
    "        edge_index = torch.tensor([sub_idxs, obj_idxs]).to(device)\n",
    "        \n",
    "#         edge_embeddings = torch.tensor(relations, dtype=torch.float).to(device)\n",
    "        edge_embeddings = relations.clone().detach().float().to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        gnndata = Data(x=node_embeddings, edge_index=edge_index, edge_attr=edge_embeddings)\n",
    "\n",
    "\n",
    "        ## return data\n",
    "        data = {\n",
    "            'image': img,\n",
    "            'depth_emb': depth_emb,\n",
    "            'depth_map': depth_map,\n",
    "            'depth': actual_depth,\n",
    "            'pooled_act_depths': pooled_act_depths,\n",
    "            'bboxs': filtered_bboxes,\n",
    "            'gnndata': gnndata\n",
    "        }\n",
    "        \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "\n",
    "        x_c, y_c, w, h = x.unbind(1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    \n",
    "        return torch.stack(b, dim=1)\n",
    "    \n",
    "    def rescale_bboxes(self, out_bbox, size):\n",
    "\n",
    "        img_w, img_h = size\n",
    "        b = self.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "        \n",
    "        b = torch.round(b).int()\n",
    "\n",
    "        b[:, 0] = torch.clamp(b[:, 0], min=0, max=img_w)\n",
    "        b[:, 1] = torch.clamp(b[:, 1], min=0, max=img_h)\n",
    "        b[:, 2] = torch.clamp(b[:, 2], min=0, max=img_w)\n",
    "        b[:, 3] = torch.clamp(b[:, 3], min=0, max=img_h)\n",
    "\n",
    "        return b\n",
    "\n",
    "    def validate_bounding_boxes(self, bboxes, img_size):\n",
    "        \"\"\"Return a list of booleans indicating whether each bounding box is valid.\"\"\"\n",
    "        valid_bboxes = []\n",
    "        img_w, img_h = img_size\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            if x1 < 0: x1 = 0\n",
    "            if y1 < 0: y1 = 0\n",
    "            if x2 > img_w: x2 = img_w\n",
    "            if y2 > img_h: y2 = img_h\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                valid_bboxes.append(True)\n",
    "            else:\n",
    "                valid_bboxes.append(False)\n",
    "\n",
    "        return valid_bboxes\n",
    "\n",
    "\n",
    "    \n",
    "    def pool_visual_content_and_depth(self, sorted_bboxes, embedding, target_size=(50, 50)):\n",
    "\n",
    "        pool = nn.AdaptiveAvgPool2d(target_size)\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            cropped_emb = embedding[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            if cropped_emb.dim() == 2:\n",
    "                cropped_emb = cropped_emb.unsqueeze(0)  \n",
    "\n",
    "            pooled_emb = pool(cropped_emb.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            flattened_emb = pooled_emb.view(-1)\n",
    "            \n",
    "            pooled_embs.append(flattened_emb)\n",
    "\n",
    "\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "    \n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "\n",
    "        x1, y1, x2, y2 = box1\n",
    "        x3, y3, x4, y4 = box2\n",
    "\n",
    "        # Calculate intersection coordinates\n",
    "        x_inter1 = max(x1, x3)\n",
    "        y_inter1 = max(y1, y3)\n",
    "        x_inter2 = min(x2, x4)\n",
    "        y_inter2 = min(y2, y4)\n",
    "\n",
    "        # Calculate intersection dimensions\n",
    "        width_inter = max(0, x_inter2 - x_inter1)\n",
    "        height_inter = max(0, y_inter2 - y_inter1)\n",
    "\n",
    "        # Calculate intersection area\n",
    "        area_inter = width_inter * height_inter\n",
    "\n",
    "        # Calculate areas of the input boxes\n",
    "        width_box1 = abs(x2 - x1)\n",
    "        height_box1 = abs(y2 - y1)\n",
    "        area_box1 = width_box1 * height_box1\n",
    "\n",
    "        width_box2 = abs(x4 - x3)\n",
    "        height_box2 = abs(y4 - y3)\n",
    "        area_box2 = width_box2 * height_box2\n",
    "\n",
    "        # Calculate union area\n",
    "        area_union = area_box1 + area_box2 - area_inter\n",
    "\n",
    "        # Calculate IoU\n",
    "        if area_union == 0:\n",
    "            return 0  # avoid division by zero\n",
    "        iou = area_inter / area_union\n",
    "\n",
    "        return iou\n",
    "    \n",
    "    \n",
    "    def assign_index(self, bounding_boxes, nodes, threshold=0.5):\n",
    "        indices = []\n",
    "        existing_boxes = nodes\n",
    "\n",
    "        for box in bounding_boxes:\n",
    "            found_match = False\n",
    "            for idx, existing_box in enumerate(existing_boxes):\n",
    "                if self.calculate_iou(box, existing_box) > threshold:\n",
    "                    indices.append(idx)\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            if not found_match:\n",
    "                existing_boxes.append(box)\n",
    "                indices.append(len(existing_boxes) - 1)\n",
    "\n",
    "        return indices, existing_boxes\n",
    "    \n",
    "    \n",
    "    def normalize(self, tensor):\n",
    "        tensor_min = tensor.min()\n",
    "        tensor_max = tensor.max()\n",
    "        normalized_tensor = (tensor - tensor_min) / ((tensor_max - tensor_min) + 1e-8)\n",
    "        return normalized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \n",
    "    \n",
    "    batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Handle images, depth maps, and other tensors separately\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    depth_embs = torch.stack([item['depth_emb'] for item in batch])\n",
    "    depth_maps = torch.stack([item['depth_map'] for item in batch])\n",
    "    depths = torch.stack([item['depth'] for item in batch])\n",
    "    \n",
    "#     pooled_act_depths = [torch.tensor(item['pooled_act_depths'], dtype=torch.int) for item in batch]\n",
    "    pooled_act_depths = [item['pooled_act_depths'].clone().detach().float() for item in batch]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     bboxs = torch.stack([item['bboxs'] for item in batch])\n",
    "    bboxs = [torch.tensor(item['bboxs'], dtype=torch.int) for item in batch]\n",
    "\n",
    "    gnndata_list = [item['gnndata'] for item in batch]\n",
    "\n",
    "    # Batch the graph data using PyTorch Geometric's Batch\n",
    "    graph_batch = Batch.from_data_list(gnndata_list)\n",
    "\n",
    "    return {\n",
    "        'image': images,\n",
    "        'depth_emb': depth_embs,\n",
    "        'depth_map': depth_maps,\n",
    "        'depth': depths,\n",
    "        'pooled_act_depths': pooled_act_depths,\n",
    "        'bboxs': bboxs,\n",
    "        'gnndata': graph_batch,  # Batched graph data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepthDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch_geometric.utils import to_networkx\n",
    "# data_list = batch['gnndata'].to_data_list()\n",
    "# # Convert and visualize each graph\n",
    "# for i, data in enumerate(data_list):\n",
    "#     G = to_networkx(data, to_undirected=True)  # Convert to networkx graph\n",
    "#     plt.figure(figsize=(8, 8))\n",
    "#     plt.title(f\"Graph {i+1}\")\n",
    "#     nx.draw(G, with_labels=True, node_size=700, node_color='lightblue')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = batch['gnndata'].to_data_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 41875])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 625])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['pooled_act_depths'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining GNN\n",
    "\n",
    "I am going to do node and edge.\n",
    "\n",
    "- node: **Zero Padding First, Then Pooling\" approach for both visual content (e.g., features extracted from bounding boxes) and depth embeddings.**\n",
    "- relationship: relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing\n",
    "# from torch_geometric.data import DataLoader, Batch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model\n",
    "class DepthGNNModel(MessagePassing):\n",
    "    def __init__(self, node_features_size, edge_features_size, hidden_channels, output_size):\n",
    "        super(DepthGNNModel, self).__init__(aggr='add')  # Aggregation: sum, mean, or max\n",
    "\n",
    "        # MLP to generate messages\n",
    "        self.message_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_features_size + edge_features_size, 2048),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),  # Dropout layer\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, hidden_channels)\n",
    "        )\n",
    "\n",
    "        # MLP to update node features\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features_size + hidden_channels, 4096),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, output_size)  # Output a flattened 25x25 depth map\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Perform message passing\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # Concatenate source node feature, target node feature, and edge feature\n",
    "        message_input = torch.cat([x_i, edge_attr, x_j], dim=-1)\n",
    "        return self.message_mlp(message_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # Concatenate original node feature with aggregated message\n",
    "        updated_node_features = torch.cat([x, aggr_out], dim=-1)\n",
    "        return self.node_mlp(updated_node_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model and move it to GPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# gnn_model = DepthGNNModel(node_features_size=82075, edge_features_size=52, hidden_channels=128, output_size=1225).to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(gnn_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DepthDataset()\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a batch of data\n",
    "batch = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4646, 0.4458, 0.4515,  ..., 0.4252, 0.4652, 0.4623],\n",
       "        [0.5198, 0.5078, 0.3987,  ..., 0.4823, 0.4704, 0.4724],\n",
       "        [0.2510, 0.1556, 0.1585,  ..., 0.3996, 0.3919, 0.4341],\n",
       "        ...,\n",
       "        [0.0618, 0.0559, 0.0465,  ..., 0.4227, 0.4281, 0.4205],\n",
       "        [0.3706, 0.3601, 0.3765,  ..., 0.4389, 0.4356, 0.4307],\n",
       "        [0.8012, 0.8081, 0.8045,  ..., 0.4357, 0.4146, 0.3980]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['gnndata'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16762202978134155\n"
     ]
    }
   ],
   "source": [
    "# def normalize(tensor):\n",
    "#     tensor_min = tensor.min()\n",
    "#     tensor_max = tensor.max()\n",
    "#     normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "#     return normalized_tensor\n",
    "\n",
    "\n",
    "\n",
    "# gnndata = batch['gnndata'].to('cuda')\n",
    "# pooled_act_depths = batch['pooled_act_depths']\n",
    "\n",
    "# # Training step for a single batch\n",
    "# gnn_model.train()\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# # Forward pass\n",
    "# output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "# # Compute the loss for each node with the corresponding ground truth in pooled_act_depths\n",
    "# loss = 0\n",
    "# for i, node_output in enumerate(output):\n",
    "# #     node_output_reshaped = node_output.view(-1, 625)\n",
    "#     ground_truth = pooled_act_depths[0][i].cuda()\n",
    "    \n",
    "#     normalized_output = normalize(node_output)\n",
    "#     normalized_ground_truth = normalize(ground_truth)\n",
    "    \n",
    "    \n",
    "#     loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "# # Average the loss\n",
    "# loss = loss / len(output)\n",
    "\n",
    "# # Backward pass\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move to cuda\n",
    "# gnndata = batch['gnndata'].to('cuda')\n",
    "# pooled_act_depths = [depth_map.cuda() for depth_map in batch['pooled_act_depths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1225])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_act_depths[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "# loss = 0\n",
    "# for i, node_output in enumerate(output):\n",
    "# #     node_output_reshaped = node_output.view(-1, output_size)\n",
    "# #     ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "#     normalized_output = (node_output - node_output.min()) / (node_output.max() - node_output.min())\n",
    "#     print(normalized_output)\n",
    "#     normalized_ground_truth = (pooled_act_depths[0][i] - pooled_act_depths[0][i].min()) / (pooled_act_depths[0][i].max() - pooled_act_depths[0][i].min())\n",
    "\n",
    "#     loss += criterion(normalized_output, normalized_ground_truth)\n",
    "#     print(loss)\n",
    "\n",
    "# loss = loss / len(output)\n",
    "\n",
    "# # Backward\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# epoch_loss += loss.item()\n",
    "\n",
    "# # Free up memory\n",
    "# del gnndata, pooled_act_depths\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Print the average loss for the epoch\n",
    "# avg_loss = epoch_loss / len(train_dataloader)\n",
    "# print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the loss\n",
    "criterion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SiLogLoss(nn.Module):\n",
    "#     def __init__(self, lambd=0.5):\n",
    "#         super().__init__()\n",
    "#         self.lambd = lambd\n",
    "\n",
    "#     def forward(self, pred, target):\n",
    "#         valid_mask = (target > 0).detach()\n",
    "#         diff_log = torch.log(target[valid_mask]) - torch.log(pred[valid_mask])\n",
    "#         loss = torch.sqrt(torch.pow(diff_log, 2).mean() -\n",
    "#                           self.lambd * torch.pow(diff_log.mean(), 2))\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './model_weights/checkpoint.pth'\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepthDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and other components\n",
    "node_features_size =  41875 # 82075 # \n",
    "edge_features_size = 52\n",
    "hidden_channels = 728\n",
    "output_size = 625 #1225 # \n",
    "\n",
    "gnn_model = DepthGNNModel(node_features_size, edge_features_size, hidden_channels, output_size).cuda()\n",
    "# torch.nn.utils.clip_grad_norm_(gnn_model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# gnn_model.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 795/795 [05:25<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Average Loss: 0.1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 795/795 [05:23<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Average Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Average Loss: 0.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 795/795 [05:27<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Average Loss: 0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 795/795 [05:23<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Average Loss: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Average Loss: 0.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 795/795 [05:24<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Average Loss: 0.1168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 795/795 [05:26<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Average Loss: 0.1157\n",
      "Checkpoint saved: checkpoint_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Average Loss: 0.1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 795/795 [05:26<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Average Loss: 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 795/795 [05:24<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Average Loss: 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 795/795 [05:25<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Average Loss: 0.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Average Loss: 0.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 795/795 [05:25<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Average Loss: 0.1112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Average Loss: 0.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Average Loss: 0.1105\n",
      "Checkpoint saved: checkpoint_epoch_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 795/795 [05:26<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Average Loss: 0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 795/795 [05:26<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Average Loss: 0.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 795/795 [05:21<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Average Loss: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 795/795 [05:22<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Average Loss: 0.1052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 795/795 [05:25<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Average Loss: 0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 795/795 [05:25<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Average Loss: 0.1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 795/795 [05:26<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Average Loss: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50:  47%|████▋     | 370/795 [02:29<03:10,  2.23it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load checkpoint if exists\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     start_epoch = load_checkpoint(gnn_model, optimizer, checkpoint_path)\n",
    "#     print(f\"Model loaded from checkpoint, starting from epoch {start_epoch + 1}\")\n",
    "# else:\n",
    "#     start_epoch = 0\n",
    "\n",
    "gnn_model.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn_model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "        # Move to cuda\n",
    "        gnndata = batch['gnndata'].to('cuda')\n",
    "        pooled_act_depths = [depth_map.cuda() for depth_map in batch['pooled_act_depths']]\n",
    "\n",
    "        output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "        loss = 0\n",
    "        for i, node_output in enumerate(output):\n",
    "            node_output_reshaped = node_output.view(-1, output_size)\n",
    "            ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "            normalized_output = (node_output_reshaped - node_output_reshaped.min()) / (node_output_reshaped.max() - node_output_reshaped.min() + 1e-8)\n",
    "            normalized_ground_truth = (ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min() + 1e-8)\n",
    "\n",
    "            loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "        loss = loss / len(output)\n",
    "        \n",
    "#         print(loss)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss became NaN. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Free up memory\n",
    "        del gnndata, pooled_act_depths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping(avg_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Save checkpoint every 8 epochs\n",
    "    if (epoch + 1) % 8 == 0:\n",
    "        checkpoint_name = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        save_checkpoint(epoch, gnn_model, optimizer, checkpoint_name)\n",
    "        print(f\"Checkpoint saved: {checkpoint_name}\")\n",
    "\n",
    "    save_checkpoint(epoch, gnn_model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(10, gnn_model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DepthDataset(mode='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "# checkpoint_path = 'checkpoint.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    gnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.cuda()\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move data to GPU\n",
    "            sub_imgs = torch.stack([item.cuda() for item in batch['sub_imgs']])\n",
    "            obj_imgs = torch.stack([item.cuda() for item in batch['obj_imgs']])\n",
    "            sub_depth_emb = torch.stack([item.cuda() for item in batch['sub_depth_emb']])\n",
    "            obj_depth_emb = torch.stack([item.cuda() for item in batch['obj_depth_emb']])\n",
    "            sub_act_depths = torch.stack([item.cuda() for item in batch['sub_act_depths']])\n",
    "            obj_act_depths = torch.stack([item.cuda() for item in batch['obj_act_depths']])\n",
    "            edges = torch.stack([item.cuda() for item in batch['relation']])\n",
    "            \n",
    "            node1_features = torch.cat([sub_imgs, sub_depth_emb], dim=-1)\n",
    "            node2_features = torch.cat([obj_imgs, obj_depth_emb], dim=-1)\n",
    "            \n",
    "            # Forward pass through the GNN model\n",
    "            depth_map1, depth_map2, updated_edges = model(node1_features, node2_features, edges)\n",
    "            \n",
    "            # Reshape the depth maps to match the ground truth dimensions if necessary\n",
    "            depth_map1 = depth_map1.view(sub_act_depths.shape)\n",
    "            depth_map2 = depth_map2.view(obj_act_depths.shape)\n",
    "            \n",
    "            # Normalize depth maps\n",
    "            normalized_depth_1 = (depth_map1 - depth_map1.min()) / (depth_map1.max() - depth_map1.min())\n",
    "            normalized_depth_2 = (depth_map2 - depth_map2.min()) / (depth_map2.max() - depth_map2.min())\n",
    "            normalized_sub_act_depths = (sub_act_depths - sub_act_depths.min()) / (sub_act_depths.max() - sub_act_depths.min())\n",
    "            normalized_obj_act_depths = (obj_act_depths - obj_act_depths.min()) / (obj_act_depths.max() - obj_act_depths.min())\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss1 = criterion(normalized_depth_1, normalized_sub_act_depths)\n",
    "            loss2 = criterion(normalized_depth_2, normalized_obj_act_depths)\n",
    "            loss = loss1 + loss2\n",
    "            \n",
    "            # Calculate MAE\n",
    "            mae1 = torch.mean(torch.abs(normalized_depth_1 - normalized_sub_act_depths))\n",
    "            mae2 = torch.mean(torch.abs(normalized_depth_2 - normalized_obj_act_depths))\n",
    "            mae = mae1 + mae2\n",
    "            \n",
    "            # Accumulate loss and MAE\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_mae = total_mae / num_batches\n",
    "    print(f\"Average Loss: {avg_loss:.4f}, Average MAE: {avg_mae:.4f}\")\n",
    "    return avg_loss, avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [02:51<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.1169, Average MAE: 0.3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11687608749893372, 0.38510026233641015)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate(gnn_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [05:22<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0964, Average MAE: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.096388840765923, 0.319969099732714)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gnn_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 795/795 [03:13<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0678, Average MAE: 0.2585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06777471745000133, 0.2584519677754468)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gnn_model, train_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Move data to GPU\n",
    "    sub_imgs = torch.stack([item.cuda() for item in batch['sub_imgs']])\n",
    "    obj_imgs = torch.stack([item.cuda() for item in batch['obj_imgs']])\n",
    "    sub_depth_emb = torch.stack([item.cuda() for item in batch['sub_depth_emb']])\n",
    "    obj_depth_emb = torch.stack([item.cuda() for item in batch['obj_depth_emb']])\n",
    "    sub_act_depths = torch.stack([item.cuda() for item in batch['sub_act_depths']])\n",
    "    obj_act_depths = torch.stack([item.cuda() for item in batch['obj_act_depths']])\n",
    "    edges = torch.stack([item.cuda() for item in batch['relation']])\n",
    "\n",
    "    node1_features = torch.cat([sub_imgs, sub_depth_emb], dim=-1)\n",
    "    node2_features = torch.cat([obj_imgs, obj_depth_emb], dim=-1)\n",
    "\n",
    "    # Forward pass through the GNN model\n",
    "    depth_map1, depth_map2, updated_edges = model(node1_features, node2_features, edges)\n",
    "\n",
    "    # Reshape the depth maps to match the ground truth dimensions if necessary\n",
    "    depth_map1 = depth_map1.view(sub_act_depths.shape)\n",
    "    depth_map2 = depth_map2.view(obj_act_depths.shape)\n",
    "\n",
    "    # Normalize depth maps\n",
    "    normalized_depth_1 = (depth_map1 - depth_map1.min()) / (depth_map1.max() - depth_map1.min())\n",
    "    normalized_depth_2 = (depth_map2 - depth_map2.min()) / (depth_map2.max() - depth_map2.min())\n",
    "    normalized_sub_act_depths = (sub_act_depths - sub_act_depths.min()) / (sub_act_depths.max() - sub_act_depths.min())\n",
    "    normalized_obj_act_depths = (obj_act_depths - obj_act_depths.min()) / (obj_act_depths.max() - obj_act_depths.min())\n",
    "\n",
    "    # Calculate loss\n",
    "    loss1 = criterion(normalized_depth_1, normalized_sub_act_depths)\n",
    "    loss2 = criterion(normalized_depth_2, normalized_obj_act_depths)\n",
    "    loss = loss1 + loss2\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae1 = torch.mean(torch.abs(normalized_depth_1 - normalized_sub_act_depths))\n",
    "    mae2 = torch.mean(torch.abs(normalized_depth_2 - normalized_obj_act_depths))\n",
    "    mae = mae1 + mae2\n",
    "\n",
    "    # Accumulate loss and MAE\n",
    "    total_loss += loss.item()\n",
    "    total_mae += mae.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "depth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
