{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNN_for_Depth.data.dataset import DepthDataset\n",
    "from GNN_for_Depth.data.utils import custom_collate\n",
    "from GNN_for_Depth.model.GNN import DepthGNNModel\n",
    "# from GNN_forDepth.utils.criterion import SiLogLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "# from torchvision.ops import RoIAlign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache to free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch_geometric.data import Data\n",
    "import h5py, torch, cv2\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/dataset/nyu_depth_v2/official_splits\", transform=None, ext=\"jpg\", mode='train', threshold=0.06):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.filenames = glob.glob(os.path.join(self.data_path, mode, '**', '*.{}'.format(ext)), recursive=True)\n",
    "        self.pt_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_embedding/nyu_depth_v2/official_splits\"\n",
    "        self.depth_map_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/depth_map/nyu_depth_v2/official_splits\"\n",
    "        self.sg_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2/official_splits\"\n",
    "\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "\n",
    "        # image path\n",
    "        img_path = self.filenames[idx]\n",
    "        # get the image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # get the relative path\n",
    "        relative_path = os.path.relpath(img_path, self.data_path)\n",
    "        # get depth embedding path\n",
    "        depth_emb_path = os.path.join(self.pt_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "        # get depth map path\n",
    "        depth_path = os.path.join(self.mode, self.depth_map_path, '{}.pt'.format(relative_path.split('.')[0]))\n",
    "\n",
    "        #get the scene graph path\n",
    "        scenegraph_path = os.path.join(self.sg_path, '{}.h5'.format(relative_path.split('.')[0]))\n",
    "\n",
    "\n",
    "\n",
    "        ## Depth Embedding\n",
    "        depth_emb = self.normalize(torch.load(depth_emb_path))\n",
    "\n",
    "        ## depth map\n",
    "        depth_map = self.normalize(torch.load(depth_path))\n",
    "\n",
    "        ## get the actual depth\n",
    "        actual_depth_path = img_path.replace(\"rgb\", \"sync_depth\").replace('.jpg', '.png')\n",
    "        actual_depth = Image.open(actual_depth_path)\n",
    "\n",
    "        \n",
    "        # the resize size\n",
    "        target_size = (25, 25)\n",
    "\n",
    "\n",
    "        with h5py.File(scenegraph_path, 'r') as h5_file:\n",
    "            loaded_output_dict = {key: torch.tensor(np.array(h5_file[key])) for key in h5_file.keys()}\n",
    "\n",
    "\n",
    "        probas = loaded_output_dict['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_sub = loaded_output_dict['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "        probas_obj = loaded_output_dict['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "        \n",
    "        \n",
    "        keep = torch.logical_and(probas.max(-1).values > self.threshold, \n",
    "                                torch.logical_and(probas_sub.max(-1).values > self.threshold, probas_obj.max(-1).values > self.threshold))\n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        sub_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['sub_boxes'][0, keep], img.size)\n",
    "        obj_bboxes_scaled = self.rescale_bboxes(loaded_output_dict['obj_boxes'][0, keep], img.size)\n",
    "        relations = loaded_output_dict['rel_logits'][0, keep]\n",
    "\n",
    "        valid_sub_bboxes = self.validate_bounding_boxes(sub_bboxes_scaled, img.size, target_size)\n",
    "        valid_obj_bboxes = self.validate_bounding_boxes(obj_bboxes_scaled, img.size, target_size)\n",
    "\n",
    "        # Combine validity of subject and object bounding boxes\n",
    "        valid_pairs = torch.tensor([vs and vo for vs, vo in zip(valid_sub_bboxes, valid_obj_bboxes)], dtype=torch.bool)\n",
    "\n",
    "        # Apply the updated keep mask\n",
    "        sub_bboxes_scaled = sub_bboxes_scaled[valid_pairs]\n",
    "        obj_bboxes_scaled = obj_bboxes_scaled[valid_pairs]\n",
    "        relations = relations[valid_pairs]\n",
    "        \n",
    "        \n",
    "        \n",
    "        sub_idxs, nodes1 = self.assign_index(sub_bboxes_scaled, [], threshold=0.7)\n",
    "        obj_idxs, nodes2 = self.assign_index(obj_bboxes_scaled, nodes1, threshold=0.7)\n",
    "        \n",
    "        all_idxs = sub_idxs + obj_idxs\n",
    "        bbox_lists = torch.concat((sub_bboxes_scaled, obj_bboxes_scaled), dim=0)\n",
    "        \n",
    "        unique_idxs = set()\n",
    "        filtered_idxs = []\n",
    "        filtered_bboxes = []\n",
    "\n",
    "        for idx, bbox in zip(all_idxs, bbox_lists):   \n",
    "            if idx not in unique_idxs:\n",
    "                unique_idxs.add(idx)\n",
    "                filtered_idxs.append(idx)\n",
    "                filtered_bboxes.append(bbox.tolist())  \n",
    "\n",
    "        # Sort the filtered indices along with their corresponding bounding boxes\n",
    "        sorted_wrapped = sorted(zip(filtered_idxs, filtered_bboxes), key=lambda x: x[0])\n",
    "        sorted_idxs, sorted_bboxes = zip(*sorted_wrapped)\n",
    "\n",
    "        # Convert them back to torch tensors\n",
    "        sorted_idxs = torch.tensor(sorted_idxs, dtype=torch.long)\n",
    "        sorted_bboxes = torch.tensor(sorted_bboxes, dtype=torch.int32)        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Apply transform to image and depth\n",
    "        img = self.transform(img)\n",
    "        img = self.normalize(img)\n",
    "        actual_depth = self.transform(actual_depth).float()\n",
    "        actual_depth = self.normalize(actual_depth)\n",
    "        \n",
    "        device = 'cpu'\n",
    "        \n",
    "        pooled_images = self.pool_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=img,\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        pooled_depths = self.pool_visual_content_and_depth(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=depth_emb[0],\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        pooled_act_depths = self.resize_depth_map(\n",
    "            sorted_bboxes=sorted_bboxes,\n",
    "            embedding=actual_depth.squeeze(0),\n",
    "            target_size=target_size).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # node embedding\n",
    "        node_embeddings = torch.cat([pooled_images, pooled_depths], dim=1).to(device)\n",
    "        \n",
    "        edge_index = torch.tensor([sub_idxs, obj_idxs]).to(device)\n",
    "        \n",
    "#         edge_embeddings = torch.tensor(relations, dtype=torch.float).to(device)\n",
    "        edge_embeddings = relations.clone().detach().float().to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        gnndata = Data(x=node_embeddings, edge_index=edge_index, edge_attr=edge_embeddings)\n",
    "\n",
    "\n",
    "        ## return data\n",
    "        data = {\n",
    "            'image': img,\n",
    "            'depth_emb': depth_emb,\n",
    "            'depth_map': depth_map,\n",
    "            'depth': actual_depth,\n",
    "            'pooled_act_depths': pooled_act_depths,\n",
    "            'bboxs': filtered_bboxes,\n",
    "            'gnndata': gnndata\n",
    "        }\n",
    "        \n",
    "        self.cache[idx] = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "\n",
    "        x_c, y_c, w, h = x.unbind(1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    \n",
    "        return torch.stack(b, dim=1)\n",
    "    \n",
    "    def rescale_bboxes(self, out_bbox, size):\n",
    "\n",
    "        img_w, img_h = size\n",
    "        b = self.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "        \n",
    "        b = torch.round(b).int()\n",
    "\n",
    "        b[:, 0] = torch.clamp(b[:, 0], min=0, max=img_w)\n",
    "        b[:, 1] = torch.clamp(b[:, 1], min=0, max=img_h)\n",
    "        b[:, 2] = torch.clamp(b[:, 2], min=0, max=img_w)\n",
    "        b[:, 3] = torch.clamp(b[:, 3], min=0, max=img_h)\n",
    "\n",
    "        return b\n",
    "\n",
    "    def validate_bounding_boxes(self, bboxes, img_size, target_size):\n",
    "        \"\"\"Return a list of booleans indicating whether each bounding box is valid.\"\"\"\n",
    "        valid_bboxes = []\n",
    "        img_w, img_h = img_size\n",
    "        \n",
    "        t_w, t_h = target_size\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            if x1 < 0: x1 = 0\n",
    "            if y1 < 0: y1 = 0\n",
    "            if x2 > img_w: x2 = img_w\n",
    "            if y2 > img_h: y2 = img_h\n",
    "            \n",
    "            if (x2 - x1) >= t_w and (y2 - y1) >= t_h:\n",
    "                valid_bboxes.append(True)\n",
    "            else:\n",
    "                valid_bboxes.append(False)\n",
    "\n",
    "        return valid_bboxes\n",
    "\n",
    "\n",
    "    \n",
    "    def pool_visual_content_and_depth(self, sorted_bboxes, embedding, target_size=(50, 50)):\n",
    "\n",
    "        pool = nn.AdaptiveAvgPool2d(target_size)\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            cropped_emb = embedding[:, bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "            if cropped_emb.dim() == 2:\n",
    "                cropped_emb = cropped_emb.unsqueeze(0)  \n",
    "\n",
    "            pooled_emb = pool(cropped_emb.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            flattened_emb = pooled_emb.view(-1)\n",
    "            \n",
    "            pooled_embs.append(flattened_emb)\n",
    "\n",
    "\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "    \n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "\n",
    "        x1, y1, x2, y2 = box1\n",
    "        x3, y3, x4, y4 = box2\n",
    "\n",
    "        # Calculate intersection coordinates\n",
    "        x_inter1 = max(x1, x3)\n",
    "        y_inter1 = max(y1, y3)\n",
    "        x_inter2 = min(x2, x4)\n",
    "        y_inter2 = min(y2, y4)\n",
    "\n",
    "        # Calculate intersection dimensions\n",
    "        width_inter = max(0, x_inter2 - x_inter1)\n",
    "        height_inter = max(0, y_inter2 - y_inter1)\n",
    "\n",
    "        # Calculate intersection area\n",
    "        area_inter = width_inter * height_inter\n",
    "\n",
    "        # Calculate areas of the input boxes\n",
    "        width_box1 = abs(x2 - x1)\n",
    "        height_box1 = abs(y2 - y1)\n",
    "        area_box1 = width_box1 * height_box1\n",
    "\n",
    "        width_box2 = abs(x4 - x3)\n",
    "        height_box2 = abs(y4 - y3)\n",
    "        area_box2 = width_box2 * height_box2\n",
    "\n",
    "        # Calculate union area\n",
    "        area_union = area_box1 + area_box2 - area_inter\n",
    "\n",
    "        # Calculate IoU\n",
    "        if area_union == 0:\n",
    "            return 0  # avoid division by zero\n",
    "        iou = area_inter / area_union\n",
    "\n",
    "        return iou\n",
    "    \n",
    "    \n",
    "    def assign_index(self, bounding_boxes, nodes, threshold=0.5):\n",
    "        indices = []\n",
    "        existing_boxes = nodes\n",
    "\n",
    "        for box in bounding_boxes:\n",
    "            found_match = False\n",
    "            for idx, existing_box in enumerate(existing_boxes):\n",
    "                if self.calculate_iou(box, existing_box) > threshold:\n",
    "                    indices.append(idx)\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            if not found_match:\n",
    "                existing_boxes.append(box)\n",
    "                indices.append(len(existing_boxes) - 1)\n",
    "\n",
    "        return indices, existing_boxes\n",
    "    \n",
    "    \n",
    "    def normalize(self, tensor):\n",
    "        tensor_min = tensor.min()\n",
    "        tensor_max = tensor.max()\n",
    "        normalized_tensor = (tensor - tensor_min) / ((tensor_max - tensor_min) + 1e-8)\n",
    "        return normalized_tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "    def resize_depth_map(self, sorted_bboxes, embedding, target_size=(25, 25)):\n",
    "\n",
    "        pooled_embs = []\n",
    "\n",
    "        for bbox in sorted_bboxes:\n",
    "            cropped_emb = embedding[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "\n",
    "\n",
    "            pooled_emb = self.downsize_depth_map_mode(cropped_emb, target_size)\n",
    "\n",
    "            flattened_emb = pooled_emb.view(-1)\n",
    "            \n",
    "            pooled_embs.append(flattened_emb)\n",
    "\n",
    "\n",
    "        pooled_embs = torch.stack(pooled_embs) if pooled_embs else torch.empty(0)\n",
    "\n",
    "        return pooled_embs\n",
    "    \n",
    "    \n",
    "    def downsize_depth_map_mode(self, depth_map, new_size):\n",
    "        \n",
    "        original_height, original_width = depth_map.shape[-2], depth_map.shape[-1]\n",
    "        new_height, new_width = new_size\n",
    "\n",
    "        window_height = original_height // new_height\n",
    "        window_width = original_width // new_width\n",
    "\n",
    "        downsized_map = torch.zeros((new_height, new_width), dtype=depth_map.dtype, device=depth_map.device)\n",
    "\n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                # Define the window boundaries\n",
    "                start_row = i * window_height\n",
    "                end_row = start_row + window_height\n",
    "                start_col = j * window_width\n",
    "                end_col = start_col + window_width\n",
    "\n",
    "                # Extract the window\n",
    "                window = depth_map[start_row:end_row, start_col:end_col]\n",
    "\n",
    "                # Flatten the window\n",
    "                flat_window = window.flatten().cpu().numpy()  # Convert to numpy for mode calculation\n",
    "\n",
    "                # Calculate the mode and handle exceptions\n",
    "                try:\n",
    "                    downsized_map[i, j] = mode(flat_window)\n",
    "                except:\n",
    "                    downsized_map[i, j] = torch.tensor(np.median(flat_window), dtype=depth_map.dtype)\n",
    "\n",
    "        return downsized_map\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \n",
    "    \n",
    "    batch = [item for item in batch if item is not None]\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Handle images, depth maps, and other tensors separately\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    depth_embs = torch.stack([item['depth_emb'] for item in batch])\n",
    "    depth_maps = torch.stack([item['depth_map'] for item in batch])\n",
    "    depths = torch.stack([item['depth'] for item in batch])\n",
    "    \n",
    "#     pooled_act_depths = [torch.tensor(item['pooled_act_depths'], dtype=torch.int) for item in batch]\n",
    "    pooled_act_depths = [item['pooled_act_depths'].clone().detach().float() for item in batch]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     bboxs = torch.stack([item['bboxs'] for item in batch])\n",
    "    bboxs = [torch.tensor(item['bboxs'], dtype=torch.int) for item in batch]\n",
    "\n",
    "    gnndata_list = [item['gnndata'] for item in batch]\n",
    "\n",
    "    # Batch the graph data using PyTorch Geometric's Batch\n",
    "    graph_batch = Batch.from_data_list(gnndata_list)\n",
    "\n",
    "    return {\n",
    "        'image': images,\n",
    "        'depth_emb': depth_embs,\n",
    "        'depth_map': depth_maps,\n",
    "        'depth': depths,\n",
    "        'pooled_act_depths': pooled_act_depths,\n",
    "        'bboxs': bboxs,\n",
    "        'gnndata': graph_batch,  # Batched graph data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepthDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch_geometric.utils import to_networkx\n",
    "# data_list = batch['gnndata'].to_data_list()\n",
    "# # Convert and visualize each graph\n",
    "# for i, data in enumerate(data_list):\n",
    "#     G = to_networkx(data, to_undirected=True)  # Convert to networkx graph\n",
    "#     plt.figure(figsize=(8, 8))\n",
    "#     plt.title(f\"Graph {i+1}\")\n",
    "#     nx.draw(G, with_labels=True, node_size=700, node_color='lightblue')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = batch['gnndata'].to_data_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 41875])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 625])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['pooled_act_depths'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining GNN\n",
    "\n",
    "I am going to do node and edge.\n",
    "\n",
    "- node: **Zero Padding First, Then Pooling\" approach for both visual content (e.g., features extracted from bounding boxes) and depth embeddings.**\n",
    "- relationship: relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn as nn\n",
    "\n",
    "class DepthGNNModel(MessagePassing):\n",
    "    def __init__(self, node_features_size, edge_features_size, hidden_channels, output_size):\n",
    "        super(DepthGNNModel, self).__init__(aggr='add')\n",
    "\n",
    "        self.message_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * node_features_size + edge_features_size, 1024),  # Reduced size\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, hidden_channels)\n",
    "        )\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_features_size + hidden_channels, 2048),  # Reduced size\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, output_size)  # Output a flattened 25x25 depth map\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        message_input = torch.cat([x_i, edge_attr, x_j], dim=-1)\n",
    "        return self.message_mlp(message_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        updated_node_features = torch.cat([x, aggr_out], dim=-1)\n",
    "        return self.node_mlp(updated_node_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model and move it to GPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# gnn_model = DepthGNNModel(node_features_size=82075, edge_features_size=52, hidden_channels=128, output_size=1225).to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(gnn_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DepthDataset()\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a batch of data\n",
    "batch = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3104, 0.3547, 0.3478,  ..., 0.4681, 0.4504, 0.4573],\n",
       "        [0.3544, 0.3539, 0.3431,  ..., 0.4329, 0.4199, 0.4193],\n",
       "        [0.0538, 0.0651, 0.0690,  ..., 0.4432, 0.4260, 0.4278],\n",
       "        ...,\n",
       "        [0.4521, 0.4317, 0.4115,  ..., 0.4586, 0.4422, 0.4296],\n",
       "        [0.3595, 0.3383, 0.3423,  ..., 0.4378, 0.4319, 0.4291],\n",
       "        [0.1691, 0.1658, 0.1478,  ..., 0.4523, 0.4557, 0.4754]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['gnndata'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(tensor):\n",
    "#     tensor_min = tensor.min()\n",
    "#     tensor_max = tensor.max()\n",
    "#     normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "#     return normalized_tensor\n",
    "\n",
    "\n",
    "\n",
    "# gnndata = batch['gnndata'].to('cuda')\n",
    "# pooled_act_depths = batch['pooled_act_depths']\n",
    "\n",
    "# # Training step for a single batch\n",
    "# gnn_model.train()\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# # Forward pass\n",
    "# output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "# # Compute the loss for each node with the corresponding ground truth in pooled_act_depths\n",
    "# loss = 0\n",
    "# for i, node_output in enumerate(output):\n",
    "# #     node_output_reshaped = node_output.view(-1, 625)\n",
    "#     ground_truth = pooled_act_depths[0][i].cuda()\n",
    "    \n",
    "#     normalized_output = normalize(node_output)\n",
    "#     normalized_ground_truth = normalize(ground_truth)\n",
    "    \n",
    "    \n",
    "#     loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "# # Average the loss\n",
    "# loss = loss / len(output)\n",
    "\n",
    "# # Backward pass\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move to cuda\n",
    "# gnndata = batch['gnndata'].to('cuda')\n",
    "# pooled_act_depths = [depth_map.cuda() for depth_map in batch['pooled_act_depths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "# loss = 0\n",
    "# for i, node_output in enumerate(output):\n",
    "# #     node_output_reshaped = node_output.view(-1, output_size)\n",
    "# #     ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "#     normalized_output = (node_output - node_output.min()) / (node_output.max() - node_output.min())\n",
    "#     print(normalized_output)\n",
    "#     normalized_ground_truth = (pooled_act_depths[0][i] - pooled_act_depths[0][i].min()) / (pooled_act_depths[0][i].max() - pooled_act_depths[0][i].min())\n",
    "\n",
    "#     loss += criterion(normalized_output, normalized_ground_truth)\n",
    "#     print(loss)\n",
    "\n",
    "# loss = loss / len(output)\n",
    "\n",
    "# # Backward\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# epoch_loss += loss.item()\n",
    "\n",
    "# # Free up memory\n",
    "# del gnndata, pooled_act_depths\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Print the average loss for the epoch\n",
    "# avg_loss = epoch_loss / len(train_dataloader)\n",
    "# print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the loss\n",
    "criterion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SiLogLoss(nn.Module):\n",
    "#     def __init__(self, lambd=0.5):\n",
    "#         super().__init__()\n",
    "#         self.lambd = lambd\n",
    "\n",
    "#     def forward(self, pred, target):\n",
    "#         valid_mask = (target > 0).detach()\n",
    "#         diff_log = torch.log(target[valid_mask]) - torch.log(pred[valid_mask])\n",
    "#         loss = torch.sqrt(torch.pow(diff_log, 2).mean() -\n",
    "#                           self.lambd * torch.pow(diff_log.mean(), 2))\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './model_weights/current_checkpoint.pth'\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepthDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and other components\n",
    "node_features_size =  41875 # 82075 # \n",
    "edge_features_size = 52\n",
    "hidden_channels = 728\n",
    "output_size = 625 #1225 # \n",
    "\n",
    "gnn_model = DepthGNNModel(node_features_size, edge_features_size, hidden_channels, output_size).cuda()\n",
    "# torch.nn.utils.clip_grad_norm_(gnn_model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# gnn_model.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    single_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn_model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "# early_stopping = EarlyStopping(patience=5, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m gnndata \u001b[38;5;241m=\u001b[39m single_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgnndata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m pooled_act_depths \u001b[38;5;241m=\u001b[39m [depth_map\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m depth_map \u001b[38;5;129;01min\u001b[39;00m single_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooled_act_depths\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnndata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgnndata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgnndata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_and_SGG_for_MDE/.depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM_and_SGG_for_MDE/.depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LLM_and_SGG_for_MDE/GNN_for_MDE/GNN_for_Depth/model/GNN.py:26\u001b[0m, in \u001b[0;36mDepthGNNModel.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/GNN_for_Depth.model.GNN_DepthGNNModel_propagate_rp5b9rvj.py:190\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    179\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    180\u001b[0m                 x_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 x\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx,\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/LLM_and_SGG_for_MDE/GNN_for_MDE/GNN_for_Depth/model/GNN.py:29\u001b[0m, in \u001b[0;36mDepthGNNModel.message\u001b[0;34m(self, x_i, x_j, edge_attr)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_i, x_j, edge_attr):\n\u001b[0;32m---> 29\u001b[0m     message_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcat([x_i, edge_attr, x_j], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_mlp(message_input)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "gnndata = single_batch['gnndata'].to('cuda')\n",
    "pooled_act_depths = [depth_map.cuda() for depth_map in single_batch['pooled_act_depths']]\n",
    "\n",
    "output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 19 is out of bounds for dimension 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output):\n\u001b[1;32m      3\u001b[0m     node_output_reshaped \u001b[38;5;241m=\u001b[39m node_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size)\n\u001b[0;32m----> 4\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mpooled_act_depths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size)\n\u001b[1;32m      6\u001b[0m     normalized_output \u001b[38;5;241m=\u001b[39m (node_output_reshaped \u001b[38;5;241m-\u001b[39m node_output_reshaped\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (node_output_reshaped\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m node_output_reshaped\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m      7\u001b[0m     normalized_ground_truth \u001b[38;5;241m=\u001b[39m (ground_truth \u001b[38;5;241m-\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (ground_truth\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 19 is out of bounds for dimension 0 with size 19"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = 0\n",
    "for i, node_output in enumerate(output):\n",
    "    node_output_reshaped = node_output.view(-1, output_size)\n",
    "    ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "    normalized_output = (node_output_reshaped - node_output_reshaped.min()) / (node_output_reshaped.max() - node_output_reshaped.min() + 1e-8)\n",
    "    normalized_ground_truth = (ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min() + 1e-8)\n",
    "\n",
    "    loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "loss = loss / len(output)\n",
    "\n",
    "if torch.isnan(loss):\n",
    "    print(\"Loss became NaN. Stopping training.\")\n",
    "\n",
    "\n",
    "# Backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# epoch_loss += loss.item()\n",
    "\n",
    "# Print the loss for the epoch\n",
    "# print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 795/795 [24:28<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Average Loss: 0.1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 795/795 [24:29<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Average Loss: 0.1430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 795/795 [24:30<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Average Loss: 0.1423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 795/795 [24:24<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Average Loss: 0.1405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 795/795 [24:36<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Average Loss: 0.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 795/795 [24:34<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Average Loss: 0.1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 795/795 [24:37<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Average Loss: 0.1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 795/795 [24:37<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Average Loss: 0.1338\n",
      "Checkpoint saved: ./model_weights/checkpoint_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 795/795 [24:36<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Average Loss: 0.1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 795/795 [24:35<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Average Loss: 0.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 795/795 [24:38<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Average Loss: 0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 795/795 [24:38<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Average Loss: 0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50:  99%|█████████▉| 788/795 [24:19<00:11,  1.64s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 14/50: 100%|██████████| 795/795 [24:32<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Average Loss: 0.1246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 795/795 [24:35<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Average Loss: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 795/795 [24:28<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Average Loss: 0.1237\n",
      "Checkpoint saved: ./model_weights/checkpoint_epoch_16.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 795/795 [24:26<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Average Loss: 0.1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 795/795 [24:26<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Average Loss: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 795/795 [24:26<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Average Loss: 0.1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 795/795 [24:25<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Average Loss: 0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 795/795 [24:40<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Average Loss: 0.1192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50:  52%|█████▏    | 416/795 [12:44<09:17,  1.47s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 31/50: 100%|██████████| 795/795 [24:22<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Average Loss: 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 795/795 [24:20<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Average Loss: 0.1136\n",
      "Checkpoint saved: ./model_weights/checkpoint_epoch_32.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 795/795 [24:20<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Average Loss: 0.1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 795/795 [24:22<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Average Loss: 0.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 795/795 [24:21<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Average Loss: 0.1131\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load checkpoint if exists\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     start_epoch = load_checkpoint(gnn_model, optimizer, checkpoint_path)\n",
    "#     print(f\"Model loaded from checkpoint, starting from epoch {start_epoch + 1}\")\n",
    "# else:\n",
    "#     start_epoch = 0\n",
    "\n",
    "gnn_model.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gnn_model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "        # Move to cuda\n",
    "        gnndata = batch['gnndata'].to('cuda')\n",
    "        pooled_act_depths = [depth_map.cuda() for depth_map in batch['pooled_act_depths']]\n",
    "\n",
    "        output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "        loss = 0\n",
    "        for i, node_output in enumerate(output):\n",
    "            node_output_reshaped = node_output.view(-1, output_size)\n",
    "            ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "            normalized_output = (node_output_reshaped - node_output_reshaped.min()) / (node_output_reshaped.max() - node_output_reshaped.min() + 1e-8)\n",
    "            normalized_ground_truth = (ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min() + 1e-8)\n",
    "\n",
    "            loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "        loss = loss / len(output)\n",
    "#         print(loss)\n",
    "        \n",
    "#         print(loss)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss became NaN. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Free up memory\n",
    "        del gnndata, pooled_act_depths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping(avg_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Save checkpoint every 8 epochs\n",
    "    if (epoch + 1) % 8 == 0:\n",
    "        checkpoint_name = f'./model_weights/checkpoint_epoch_{epoch+1}.pth'\n",
    "        save_checkpoint(epoch, gnn_model, optimizer, checkpoint_name)\n",
    "        print(f\"Checkpoint saved: {checkpoint_name}\")\n",
    "\n",
    "    save_checkpoint(epoch, gnn_model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 14 18:41:04 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:49:00.0 Off |                   On |\n",
      "| N/A   36C    P0            107W /  300W |    6603MiB /  81920MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                            |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                  |        ECC|                       |\n",
      "|==================+==================================+===========+=======================|\n",
      "|  0   10   0   0  |            3894MiB /  9728MiB    | 14      0 |  1   0    0    0    0 |\n",
      "|                  |                 2MiB / 16383MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   10    0     955991      C   ..._and_SGG_for_MDE/.depth/bin/python3       3874MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(10, gnn_model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DepthDataset(mode='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "# checkpoint_path = 'checkpoint.pth'\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "#     gnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     print(\"Model loaded from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint, starting evaluation from epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [27:52<00:00,  2.56s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eval_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print the average loss for the evaluation\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43meval_dataloader\u001b[49m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Loss during Evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Load checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(gnn_model, optimizer, checkpoint_path)\n",
    "    print(f\"Model loaded from checkpoint, starting evaluation from epoch {start_epoch + 1}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Checkpoint not found. Ensure the correct checkpoint path is provided.\")\n",
    "\n",
    "gnn_model.eval()  # Switch to evaluation mode\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "\n",
    "        # Move to cuda\n",
    "        gnndata = batch['gnndata'].to('cuda')\n",
    "        pooled_act_depths = [depth_map.cuda() for depth_map in batch['pooled_act_depths']]\n",
    "\n",
    "        output = gnn_model(gnndata.x, gnndata.edge_index, gnndata.edge_attr)\n",
    "\n",
    "        loss = 0\n",
    "        for i, node_output in enumerate(output):\n",
    "            node_output_reshaped = node_output.view(-1, output_size)\n",
    "            ground_truth = pooled_act_depths[0][i].view(-1, output_size)\n",
    "\n",
    "            normalized_output = (node_output_reshaped - node_output_reshaped.min()) / (node_output_reshaped.max() - node_output_reshaped.min() + 1e-8)\n",
    "            normalized_ground_truth = (ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min() + 1e-8)\n",
    "\n",
    "            loss += criterion(normalized_output, normalized_ground_truth)\n",
    "\n",
    "        loss = loss / len(output)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss became NaN during evaluation. Stopping evaluation.\")\n",
    "            break\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Free up memory\n",
    "        del gnndata, pooled_act_depths\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Print the average loss for the evaluation\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    print(f\"Average Loss during Evaluation: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss during Evaluation: 0.1133\n"
     ]
    }
   ],
   "source": [
    "avg_loss = total_loss / len(test_dataloader)\n",
    "print(f\"Average Loss during Evaluation: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [02:51<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.1169, Average MAE: 0.3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11687608749893372, 0.38510026233641015)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate(gnn_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 654/654 [05:22<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0964, Average MAE: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.096388840765923, 0.319969099732714)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gnn_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 795/795 [03:13<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0678, Average MAE: 0.2585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06777471745000133, 0.2584519677754468)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gnn_model, train_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Move data to GPU\n",
    "    sub_imgs = torch.stack([item.cuda() for item in batch['sub_imgs']])\n",
    "    obj_imgs = torch.stack([item.cuda() for item in batch['obj_imgs']])\n",
    "    sub_depth_emb = torch.stack([item.cuda() for item in batch['sub_depth_emb']])\n",
    "    obj_depth_emb = torch.stack([item.cuda() for item in batch['obj_depth_emb']])\n",
    "    sub_act_depths = torch.stack([item.cuda() for item in batch['sub_act_depths']])\n",
    "    obj_act_depths = torch.stack([item.cuda() for item in batch['obj_act_depths']])\n",
    "    edges = torch.stack([item.cuda() for item in batch['relation']])\n",
    "\n",
    "    node1_features = torch.cat([sub_imgs, sub_depth_emb], dim=-1)\n",
    "    node2_features = torch.cat([obj_imgs, obj_depth_emb], dim=-1)\n",
    "\n",
    "    # Forward pass through the GNN model\n",
    "    depth_map1, depth_map2, updated_edges = model(node1_features, node2_features, edges)\n",
    "\n",
    "    # Reshape the depth maps to match the ground truth dimensions if necessary\n",
    "    depth_map1 = depth_map1.view(sub_act_depths.shape)\n",
    "    depth_map2 = depth_map2.view(obj_act_depths.shape)\n",
    "\n",
    "    # Normalize depth maps\n",
    "    normalized_depth_1 = (depth_map1 - depth_map1.min()) / (depth_map1.max() - depth_map1.min())\n",
    "    normalized_depth_2 = (depth_map2 - depth_map2.min()) / (depth_map2.max() - depth_map2.min())\n",
    "    normalized_sub_act_depths = (sub_act_depths - sub_act_depths.min()) / (sub_act_depths.max() - sub_act_depths.min())\n",
    "    normalized_obj_act_depths = (obj_act_depths - obj_act_depths.min()) / (obj_act_depths.max() - obj_act_depths.min())\n",
    "\n",
    "    # Calculate loss\n",
    "    loss1 = criterion(normalized_depth_1, normalized_sub_act_depths)\n",
    "    loss2 = criterion(normalized_depth_2, normalized_obj_act_depths)\n",
    "    loss = loss1 + loss2\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae1 = torch.mean(torch.abs(normalized_depth_1 - normalized_sub_act_depths))\n",
    "    mae2 = torch.mean(torch.abs(normalized_depth_2 - normalized_obj_act_depths))\n",
    "    mae = mae1 + mae2\n",
    "\n",
    "    # Accumulate loss and MAE\n",
    "    total_loss += loss.item()\n",
    "    total_mae += mae.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "depth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
