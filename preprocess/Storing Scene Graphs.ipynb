{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100 80GB PCIe MIG 1g.10gb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RelTR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn_entity): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2_entity): Dropout(p=0.1, inplace=False)\n",
       "          (norm2_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_entity): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1_entity): Dropout(p=0.1, inplace=False)\n",
       "          (norm1_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_so): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2_so): Dropout(p=0.1, inplace=False)\n",
       "          (norm2_so): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_sub): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1_sub): Dropout(p=0.1, inplace=False)\n",
       "          (norm1_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_sub_entity): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2_sub): Dropout(p=0.1, inplace=False)\n",
       "          (norm2_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_obj): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1_obj): Dropout(p=0.1, inplace=False)\n",
       "          (norm1_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_obj_entity): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2_obj): Dropout(p=0.1, inplace=False)\n",
       "          (norm2_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1_entity): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3_entity): Dropout(p=0.1, inplace=False)\n",
       "          (linear2_entity): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4_entity): Dropout(p=0.1, inplace=False)\n",
       "          (norm3_entity): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1_sub): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3_sub): Dropout(p=0.1, inplace=False)\n",
       "          (linear2_sub): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4_sub): Dropout(p=0.1, inplace=False)\n",
       "          (norm3_sub): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1_obj): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout3_obj): Dropout(p=0.1, inplace=False)\n",
       "          (linear2_obj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout4_obj): Dropout(p=0.1, inplace=False)\n",
       "          (norm3_obj): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       "  (entity_embed): Embedding(100, 512)\n",
       "  (triplet_embed): Embedding(200, 768)\n",
       "  (so_embed): Embedding(2, 256)\n",
       "  (entity_class_embed): Linear(in_features=256, out_features=152, bias=True)\n",
       "  (entity_bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (so_mask_conv): Sequential(\n",
       "    (0): Upsample(size=(28, 28), mode='nearest')\n",
       "    (1): Conv2d(2, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (so_mask_fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (rel_class_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=640, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=52, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sub_class_embed): Linear(in_features=256, out_features=152, bias=True)\n",
       "  (sub_bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (obj_class_embed): Linear(in_features=256, out_features=152, bias=True)\n",
       "  (obj_bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.backbone import Backbone, Joiner\n",
    "from models.position_encoding import PositionEmbeddingSine\n",
    "from models.transformer import Transformer\n",
    "from models.reltr import RelTR\n",
    "\n",
    "position_embedding = PositionEmbeddingSine(128, normalize=True)\n",
    "backbone = Backbone('resnet50', False, False, False)\n",
    "backbone = Joiner(backbone, position_embedding)\n",
    "backbone.num_channels = 2048\n",
    "\n",
    "transformer = Transformer(d_model=256, dropout=0.1, nhead=8,\n",
    "                          dim_feedforward=2048,\n",
    "                          num_encoder_layers=6,\n",
    "                          num_decoder_layers=6,\n",
    "                          normalize_before=False,\n",
    "                          return_intermediate_dec=True)\n",
    "\n",
    "model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n",
    "              num_entities=100, num_triplets=200)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "\n",
    "model_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/SGG/RelTR/pretrained/checkpoint0149.pth\" \n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some transformation functions\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    print(out_bbox.shape)\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "data_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/dataset/nyu_depth_v2\"\n",
    "save_path = \"/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2\"\n",
    "\n",
    "# Check if save_path exists\n",
    "if not os.path.exists(save_path):\n",
    "    print(f\"Creating directory: {save_path}\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directory already exists: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [03:26<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jpg_files = glob.glob(os.path.join(data_path, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for img_path in tqdm(jpg_files, total=len(jpg_files)):\n",
    "\n",
    "    relative_path = os.path.relpath(img_path, data_path)\n",
    "    h5_filename = os.path.splitext(relative_path)[0] + '.h5'\n",
    "    h5_file_path = os.path.join(save_path, h5_filename)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(h5_file_path), exist_ok=True)\n",
    "    \n",
    "    \n",
    "    im = Image.open(img_path)\n",
    "    img = transform(im).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    outputs = model(img)\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'w') as h5_file:\n",
    "\n",
    "        for key, value in outputs.items():\n",
    "            h5_file.create_dataset(key, data=value.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1342823/4240642520.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  outputs = {key: torch.tensor(f[key]) for key in f.keys()}\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/home3/fsml62/LLM_and_SGG_for_MDE/GNN_for_MDE/results/SGG/nyu_depth_v2/official_splits/train/printer_room/rgb_00448.h5', 'r') as f:\n",
    "    outputs = {key: torch.tensor(f[key]) for key in f.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2887, 0.3825, 0.2953, 0.1416],\n",
       "        [0.2369, 0.2324, 0.0598, 0.1156],\n",
       "        [0.6234, 0.7847, 0.4410, 0.4206],\n",
       "        [0.0777, 0.3373, 0.1512, 0.1748],\n",
       "        [0.6441, 0.4414, 0.3045, 0.1077],\n",
       "        [0.5009, 0.4992, 0.9995, 0.9985],\n",
       "        [0.0760, 0.3371, 0.1489, 0.1756],\n",
       "        [0.2634, 0.4531, 0.2592, 0.0940],\n",
       "        [0.6231, 0.7856, 0.4509, 0.4204],\n",
       "        [0.2730, 0.4532, 0.2803, 0.1177],\n",
       "        [0.6163, 0.7848, 0.4474, 0.4170],\n",
       "        [0.0863, 0.5870, 0.1751, 0.2838],\n",
       "        [0.6218, 0.7845, 0.4448, 0.4207],\n",
       "        [0.2994, 0.3937, 0.2841, 0.1867],\n",
       "        [0.6211, 0.7835, 0.4458, 0.4244],\n",
       "        [0.6207, 0.7860, 0.4367, 0.4150],\n",
       "        [0.9219, 0.1810, 0.0454, 0.2193],\n",
       "        [0.0881, 0.5886, 0.1771, 0.2877],\n",
       "        [0.2225, 0.6882, 0.1039, 0.1054],\n",
       "        [0.0886, 0.5894, 0.1800, 0.2883],\n",
       "        [0.2842, 0.6823, 0.3241, 0.4025],\n",
       "        [0.2350, 0.2325, 0.0584, 0.1129],\n",
       "        [0.0876, 0.5873, 0.1780, 0.2917],\n",
       "        [0.0843, 0.5912, 0.1708, 0.2902],\n",
       "        [0.2819, 0.4242, 0.3296, 0.2480],\n",
       "        [0.0887, 0.5877, 0.1762, 0.2837],\n",
       "        [0.9207, 0.1807, 0.0426, 0.2175],\n",
       "        [0.9234, 0.1780, 0.0467, 0.2195],\n",
       "        [0.2395, 0.2364, 0.0629, 0.1228],\n",
       "        [0.2833, 0.4243, 0.3252, 0.2422],\n",
       "        [0.6226, 0.7876, 0.4435, 0.4137],\n",
       "        [0.6242, 0.7832, 0.4358, 0.4222],\n",
       "        [0.2805, 0.8438, 0.0605, 0.0835],\n",
       "        [0.9399, 0.2880, 0.1167, 0.5405],\n",
       "        [0.2740, 0.8443, 0.0615, 0.0771],\n",
       "        [0.2815, 0.4236, 0.3247, 0.2449],\n",
       "        [0.2833, 0.6743, 0.3194, 0.4151],\n",
       "        [0.0875, 0.5867, 0.1783, 0.2831],\n",
       "        [0.8279, 0.4858, 0.0548, 0.1096],\n",
       "        [0.9223, 0.1766, 0.0465, 0.2178],\n",
       "        [0.2744, 0.8440, 0.0591, 0.0768],\n",
       "        [0.8287, 0.4853, 0.0561, 0.1174],\n",
       "        [0.2763, 0.8373, 0.0608, 0.0853],\n",
       "        [0.6486, 0.4442, 0.2867, 0.1040],\n",
       "        [0.2829, 0.4264, 0.3282, 0.2422],\n",
       "        [0.6286, 0.5048, 0.4329, 0.2267],\n",
       "        [0.2828, 0.6780, 0.3210, 0.4044],\n",
       "        [0.6325, 0.5003, 0.4234, 0.2226],\n",
       "        [0.6245, 0.7772, 0.4361, 0.4296],\n",
       "        [0.6352, 0.4624, 0.4011, 0.1563],\n",
       "        [0.2847, 0.6811, 0.3183, 0.4014],\n",
       "        [0.2724, 0.4395, 0.2877, 0.1353],\n",
       "        [0.9132, 0.6839, 0.1726, 0.3701],\n",
       "        [0.2982, 0.3710, 0.2790, 0.1237],\n",
       "        [0.2330, 0.2366, 0.0654, 0.1154],\n",
       "        [0.2243, 0.6959, 0.0998, 0.1092],\n",
       "        [0.2765, 0.4619, 0.3221, 0.1436],\n",
       "        [0.4739, 0.9423, 0.2023, 0.1023],\n",
       "        [0.6443, 0.4453, 0.3172, 0.1138],\n",
       "        [0.2821, 0.4267, 0.3193, 0.2469],\n",
       "        [0.6279, 0.7890, 0.4368, 0.4141],\n",
       "        [0.2838, 0.4128, 0.3121, 0.2199],\n",
       "        [0.4934, 0.9618, 0.1795, 0.0666],\n",
       "        [0.6224, 0.7888, 0.4426, 0.4105],\n",
       "        [0.0743, 0.3389, 0.1462, 0.1774],\n",
       "        [0.8324, 0.4859, 0.0519, 0.1123],\n",
       "        [0.2872, 0.6750, 0.3254, 0.4115],\n",
       "        [0.6310, 0.4719, 0.4108, 0.1724],\n",
       "        [0.2160, 0.6976, 0.1132, 0.1136],\n",
       "        [0.2794, 0.4272, 0.3243, 0.2412],\n",
       "        [0.6210, 0.7841, 0.4408, 0.4168],\n",
       "        [0.6325, 0.4951, 0.4234, 0.2149],\n",
       "        [0.0729, 0.3377, 0.1395, 0.1707],\n",
       "        [0.0889, 0.5849, 0.1796, 0.2864],\n",
       "        [0.6351, 0.4810, 0.4001, 0.1853],\n",
       "        [0.9230, 0.1814, 0.0498, 0.2239],\n",
       "        [0.6438, 0.4425, 0.3358, 0.1103],\n",
       "        [0.6154, 0.8050, 0.4267, 0.3799],\n",
       "        [0.5262, 0.7881, 0.2388, 0.3845],\n",
       "        [0.2825, 0.4222, 0.3202, 0.2438],\n",
       "        [0.6442, 0.4422, 0.2659, 0.1010],\n",
       "        [0.6234, 0.7848, 0.4437, 0.4191],\n",
       "        [0.5212, 0.7918, 0.1881, 0.3627],\n",
       "        [0.6384, 0.4513, 0.3681, 0.1288],\n",
       "        [0.2843, 0.6763, 0.3229, 0.4112],\n",
       "        [0.2364, 0.2314, 0.0573, 0.1146],\n",
       "        [0.2165, 0.6958, 0.1131, 0.1134],\n",
       "        [0.2810, 0.6724, 0.3237, 0.4149],\n",
       "        [0.2563, 0.4477, 0.2350, 0.0797],\n",
       "        [0.0898, 0.5843, 0.1811, 0.2841],\n",
       "        [0.6187, 0.7965, 0.4348, 0.3910],\n",
       "        [0.6200, 0.7833, 0.4420, 0.4158],\n",
       "        [0.0895, 0.5861, 0.1819, 0.2855],\n",
       "        [0.9223, 0.1799, 0.0469, 0.2259],\n",
       "        [0.2205, 0.6945, 0.1065, 0.1096],\n",
       "        [0.6195, 0.7834, 0.4390, 0.4130],\n",
       "        [0.2803, 0.4239, 0.3273, 0.2452],\n",
       "        [0.6319, 0.5038, 0.4344, 0.2285],\n",
       "        [0.6247, 0.7810, 0.4430, 0.4274],\n",
       "        [0.6215, 0.7839, 0.4416, 0.4219],\n",
       "        [0.0759, 0.3398, 0.1505, 0.1780],\n",
       "        [0.2837, 0.6874, 0.3189, 0.3945],\n",
       "        [0.6326, 0.4945, 0.4219, 0.1989],\n",
       "        [0.6322, 0.4828, 0.4076, 0.1967],\n",
       "        [0.2979, 0.3713, 0.2724, 0.1300],\n",
       "        [0.2768, 0.4268, 0.3166, 0.1950],\n",
       "        [0.2831, 0.4245, 0.3247, 0.2442],\n",
       "        [0.2773, 0.8404, 0.0566, 0.0823],\n",
       "        [0.2760, 0.4216, 0.2988, 0.1375],\n",
       "        [0.2808, 0.6762, 0.3272, 0.4123],\n",
       "        [0.6350, 0.4723, 0.3903, 0.1696],\n",
       "        [0.5008, 0.4992, 0.9999, 0.9990],\n",
       "        [0.2874, 0.6729, 0.3247, 0.4170],\n",
       "        [0.6230, 0.7840, 0.4402, 0.4200],\n",
       "        [0.2233, 0.6968, 0.1022, 0.1104],\n",
       "        [0.6226, 0.7828, 0.4367, 0.4201],\n",
       "        [0.2828, 0.6660, 0.3273, 0.4045],\n",
       "        [0.2800, 0.8412, 0.0579, 0.0826],\n",
       "        [0.0948, 0.4450, 0.1836, 0.0834],\n",
       "        [0.6213, 0.7800, 0.4452, 0.4310],\n",
       "        [0.2523, 0.6907, 0.0845, 0.1075],\n",
       "        [0.6376, 0.4494, 0.3854, 0.1336],\n",
       "        [0.0925, 0.5828, 0.1847, 0.2819],\n",
       "        [0.6042, 0.8371, 0.0385, 0.2952],\n",
       "        [0.2619, 0.4426, 0.2515, 0.0842],\n",
       "        [0.2786, 0.8402, 0.0623, 0.0856],\n",
       "        [0.9232, 0.1789, 0.0419, 0.2132],\n",
       "        [0.2715, 0.4843, 0.3268, 0.1770],\n",
       "        [0.6205, 0.7908, 0.4383, 0.4072],\n",
       "        [0.2838, 0.4249, 0.3220, 0.2472],\n",
       "        [0.9242, 0.1794, 0.0433, 0.2108],\n",
       "        [0.2856, 0.6893, 0.3203, 0.3931],\n",
       "        [0.0897, 0.5847, 0.1819, 0.2869],\n",
       "        [0.6337, 0.4871, 0.4078, 0.1969],\n",
       "        [0.6406, 0.4482, 0.3574, 0.1231],\n",
       "        [0.6301, 0.5064, 0.4277, 0.1957],\n",
       "        [0.2776, 0.4346, 0.3282, 0.2235],\n",
       "        [0.2823, 0.4216, 0.3076, 0.1769],\n",
       "        [0.6216, 0.7975, 0.4371, 0.3952],\n",
       "        [0.5008, 0.4995, 0.9997, 0.9988],\n",
       "        [0.2353, 0.2368, 0.0624, 0.1157],\n",
       "        [0.6374, 0.4605, 0.3976, 0.1553],\n",
       "        [0.0735, 0.3375, 0.1424, 0.1729],\n",
       "        [0.0747, 0.3379, 0.1456, 0.1730],\n",
       "        [0.0915, 0.5820, 0.1845, 0.2863],\n",
       "        [0.0887, 0.5866, 0.1811, 0.2944],\n",
       "        [0.6235, 0.7854, 0.4402, 0.4200],\n",
       "        [0.0787, 0.3405, 0.1502, 0.1794],\n",
       "        [0.9231, 0.1787, 0.0508, 0.2301],\n",
       "        [0.3058, 0.3729, 0.2610, 0.1272],\n",
       "        [0.0898, 0.5854, 0.1821, 0.2930],\n",
       "        [0.2829, 0.4215, 0.3251, 0.2436],\n",
       "        [0.6388, 0.4411, 0.3685, 0.1142],\n",
       "        [0.0887, 0.5864, 0.1781, 0.2895],\n",
       "        [0.9239, 0.1786, 0.0447, 0.2079],\n",
       "        [0.6186, 0.7883, 0.4167, 0.4011],\n",
       "        [0.0868, 0.5884, 0.1755, 0.2867],\n",
       "        [0.9224, 0.1788, 0.0454, 0.2199],\n",
       "        [0.2795, 0.4257, 0.3278, 0.2477],\n",
       "        [0.1035, 0.3312, 0.1454, 0.1619],\n",
       "        [0.5183, 0.7955, 0.2186, 0.3779],\n",
       "        [0.2342, 0.2322, 0.0590, 0.1181],\n",
       "        [0.0874, 0.5880, 0.1776, 0.2864],\n",
       "        [0.8313, 0.4848, 0.0515, 0.1081],\n",
       "        [0.2832, 0.6743, 0.3263, 0.4141],\n",
       "        [0.9213, 0.1794, 0.0449, 0.2192],\n",
       "        [0.0755, 0.3387, 0.1459, 0.1748],\n",
       "        [0.2778, 0.8415, 0.0614, 0.0779],\n",
       "        [0.0736, 0.3424, 0.1426, 0.1789],\n",
       "        [0.2833, 0.4185, 0.3216, 0.2348],\n",
       "        [0.2823, 0.6818, 0.3247, 0.4057],\n",
       "        [0.2952, 0.3633, 0.2772, 0.1078],\n",
       "        [0.0776, 0.3391, 0.1531, 0.1785],\n",
       "        [0.6421, 0.4464, 0.3266, 0.1173],\n",
       "        [0.2638, 0.6825, 0.0804, 0.1071],\n",
       "        [0.2766, 0.8407, 0.0583, 0.0815],\n",
       "        [0.3043, 0.3552, 0.2465, 0.0900],\n",
       "        [0.2395, 0.2343, 0.0616, 0.1120],\n",
       "        [0.9243, 0.1786, 0.0435, 0.2249],\n",
       "        [0.2341, 0.2372, 0.0626, 0.1144],\n",
       "        [0.9378, 0.2845, 0.1187, 0.5318],\n",
       "        [0.6322, 0.5036, 0.4229, 0.2161],\n",
       "        [0.6202, 0.7854, 0.4396, 0.4188],\n",
       "        [0.2811, 0.4242, 0.3225, 0.2394],\n",
       "        [0.6193, 0.7807, 0.4441, 0.4265],\n",
       "        [0.0908, 0.5845, 0.1820, 0.2910],\n",
       "        [0.2762, 0.8441, 0.0598, 0.0785],\n",
       "        [0.9233, 0.1784, 0.0447, 0.2233],\n",
       "        [0.2807, 0.6676, 0.3287, 0.4189],\n",
       "        [0.2821, 0.4233, 0.3284, 0.2374],\n",
       "        [0.2162, 0.6988, 0.1069, 0.1074],\n",
       "        [0.9249, 0.1808, 0.0508, 0.2237],\n",
       "        [0.6223, 0.7747, 0.4461, 0.4418],\n",
       "        [0.6266, 0.7876, 0.4384, 0.4058],\n",
       "        [0.6380, 0.4717, 0.3957, 0.1697],\n",
       "        [0.6219, 0.7882, 0.4410, 0.4138],\n",
       "        [0.2697, 0.4434, 0.2849, 0.1220],\n",
       "        [0.0738, 0.3383, 0.1442, 0.1753],\n",
       "        [0.2332, 0.2344, 0.0594, 0.1153],\n",
       "        [0.2818, 0.6621, 0.3245, 0.4213]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sub_boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2953, 0.0598, 0.4410, 0.1512, 0.3045, 0.9995, 0.1489, 0.2592, 0.4509,\n",
       "        0.2803, 0.4474, 0.1751, 0.4448, 0.2841, 0.4458, 0.4367, 0.0454, 0.1771,\n",
       "        0.1039, 0.1800, 0.3241, 0.0584, 0.1780, 0.1708, 0.3296, 0.1762, 0.0426,\n",
       "        0.0467, 0.0629, 0.3252, 0.4435, 0.4358, 0.0605, 0.1167, 0.0615, 0.3247,\n",
       "        0.3194, 0.1783, 0.0548, 0.0465, 0.0591, 0.0561, 0.0608, 0.2867, 0.3282,\n",
       "        0.4329, 0.3210, 0.4234, 0.4361, 0.4011, 0.3183, 0.2877, 0.1726, 0.2790,\n",
       "        0.0654, 0.0998, 0.3221, 0.2023, 0.3172, 0.3193, 0.4368, 0.3121, 0.1795,\n",
       "        0.4426, 0.1462, 0.0519, 0.3254, 0.4108, 0.1132, 0.3243, 0.4408, 0.4234,\n",
       "        0.1395, 0.1796, 0.4001, 0.0498, 0.3358, 0.4267, 0.2388, 0.3202, 0.2659,\n",
       "        0.4437, 0.1881, 0.3681, 0.3229, 0.0573, 0.1131, 0.3237, 0.2350, 0.1811,\n",
       "        0.4348, 0.4420, 0.1819, 0.0469, 0.1065, 0.4390, 0.3273, 0.4344, 0.4430,\n",
       "        0.4416, 0.1505, 0.3189, 0.4219, 0.4076, 0.2724, 0.3166, 0.3247, 0.0566,\n",
       "        0.2988, 0.3272, 0.3903, 0.9999, 0.3247, 0.4402, 0.1022, 0.4367, 0.3273,\n",
       "        0.0579, 0.1836, 0.4452, 0.0845, 0.3854, 0.1847, 0.0385, 0.2515, 0.0623,\n",
       "        0.0419, 0.3268, 0.4383, 0.3220, 0.0433, 0.3203, 0.1819, 0.4078, 0.3574,\n",
       "        0.4277, 0.3282, 0.3076, 0.4371, 0.9997, 0.0624, 0.3976, 0.1424, 0.1456,\n",
       "        0.1845, 0.1811, 0.4402, 0.1502, 0.0508, 0.2610, 0.1821, 0.3251, 0.3685,\n",
       "        0.1781, 0.0447, 0.4167, 0.1755, 0.0454, 0.3278, 0.1454, 0.2186, 0.0590,\n",
       "        0.1776, 0.0515, 0.3263, 0.0449, 0.1459, 0.0614, 0.1426, 0.3216, 0.3247,\n",
       "        0.2772, 0.1531, 0.3266, 0.0804, 0.0583, 0.2465, 0.0616, 0.0435, 0.0626,\n",
       "        0.1187, 0.4229, 0.4396, 0.3225, 0.4441, 0.1820, 0.0598, 0.0447, 0.3287,\n",
       "        0.3284, 0.1069, 0.0508, 0.4461, 0.4384, 0.3957, 0.4410, 0.2849, 0.1442,\n",
       "        0.0594, 0.3245])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sub_boxes'][0,:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# keep only predictions with >0.3 confidence\n",
    "probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = torch.logical_and(probas.max(-1).values > 0.3, torch.logical_and(probas_sub.max(-1).values > 0.3,\n",
    "                                                                        probas_obj.max(-1).values > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# convert boxes from [0; 1] to image scales\n",
    "sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n",
    "obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n",
    "rel_list = torch.argmax(outputs['rel_logits'][0, keep], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "\n",
    "keep = torch.logical_and(probas.max(-1).values > 0.3,\n",
    "                         torch.logical_and(probas_sub.max(-1).values > 0.1,\n",
    "                                           probas_obj.max(-1).values > 0.1))\n",
    "\n",
    "\n",
    "sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n",
    "obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n",
    "rel_list = torch.argmax(outputs['rel_logits'][0, keep], dim=1)\n",
    "\n",
    "\n",
    "# Store the filtered outputs in the dictionary\n",
    "filtered_data = {\n",
    "    'probas': probas[keep],\n",
    "    'probas_sub': probas_sub[keep],\n",
    "    'probas_obj': probas_obj[keep]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 51])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['probas'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 79.8119, 216.0722, 287.6272, 426.2114],\n",
       "        [ 77.8434, 216.6798, 286.8960, 424.1441],\n",
       "        [ 66.4892, 193.3925, 235.2505, 243.1502],\n",
       "        [ 72.4065, 184.8528, 285.2302, 269.1777],\n",
       "        [ 77.5479, 223.8046, 287.3661, 431.0623],\n",
       "        [ 72.3060, 188.4806, 287.1606, 270.6227]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_bboxes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 52])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['rel_logits'][0, keep].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "depth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
